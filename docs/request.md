# Send request

- [Application code](#application-code)
- [Client stub](#client-stub)
- [Fork road](#fork-road)
- [Send Request-Headers](#send-request-headers)
  - [Pick stream transport](#pick-stream-transport)
  - [Send header](#send-header)
- [Send Length-Prefixed-Message and EOS](#send-length-prefixed-message-and-eos)
- [Receive response](#receive-response)

gRPC uses HTTP 2 frames to send request and process response. But how to do that exactly? let's explain the detail implementation of gRPC request. [gRPC over HTTP2](https://github.com/grpc/grpc/blob/master/doc/PROTOCOL-HTTP2.md) is a good start point to explain the design of gRPC. In brief, the gRPC request is transformed into three parts:

```txt
Request → Request-Headers *Length-Prefixed-Message EOS. 
```

- a header frame (`Request-Headers`),
- zero or more data Frame (`Length-Prefixed-Message`),
- the final part is `EOS` (end of stream) is a flag, set in the last data frame.

The following is the gRPC request client side sequence diagram. It focus on sending gRPC request: mainly `Request-Headers` and `Length-Prefixed-Message`.

- Yellow box represents the important type and method/function.
- Arrow represents the call direction and order.
- Right red dot means there is another map for that box.

![images/images.003.png](../images/images.003.png)

## Application code

Here is the gRPC client application code snippet. It calls `pb.NewGreeterClient()` to create the client stub and calls `c.SayHello()` to send the request over HTTP 2. Pretty simple, right?

Please note the connections to the target server is already established by `grpc.Dial()`. `c.SayHello()` will use one of the connections to send the request. See [Client Dial](dial.md) for detail.

Let's continue to see what happens under the hood.

```go
const (
    address     = "localhost:50051"
    defaultName = "world"
)

func main() {
    // Set up a connection to the server.
    conn, err := grpc.Dial(address, grpc.WithInsecure(), grpc.WithBlock())
    if err != nil {
        log.Fatalf("did not connect: %v", err)
    }
    defer conn.Close()
    c := pb.NewGreeterClient(conn)

    // Contact the server and print out its response.
    name := defaultName
    if len(os.Args) > 1 {
        name = os.Args[1]
    }
    ctx, cancel := context.WithTimeout(context.Background(), time.Second)
    defer cancel()
    r, err := c.SayHello(ctx, &pb.HelloRequest{Name: name})
    if err != nil {
        log.Fatalf("could not greet: %v", err)
    }
    log.Printf("Greeting: %s", r.GetMessage())
}
```

## Client stub

`greeterClient` is the gRPC client stub generated by gRPC. In this stub,

- `SayHello()` calls `c.cc.Invoke()`, which is actually `ClientConn.Invoke()`.
- `ClientConn.Invoke()` is a general method which can be used by other client stub.
- Please note the `method` parameter of `ClientConn.Invoke()` has the fix argument value `"/helloworld.Greeter/SayHello"`.

Please refer to the definition of method parameter( the canonical name of `"/helloworld.Greeter/SayHello"`) to understand the string meaning :

```txt
* Path → ":path" "/" Service-Name "/" {method name}
* Service-Name → {IDL-specific service name}
```

```go
func (c *greeterClient) SayHello(ctx context.Context, in *HelloRequest, opts ...grpc.CallOption) (*HelloReply, error) {
    out := new(HelloReply)                 
    err := c.cc.Invoke(ctx, "/helloworld.Greeter/SayHello", in, out, opts...)      
    if err != nil {                           
        return nil, err
    }
    return out, nil
}
```

- `Invoke()` combines the `cc.dopts.callOptions` first.
- If there is any interceptor `cc.dopts.unaryInt != nil`, use the interceptor to send the request.
- Otherwise directly call the `invoke()` function.
- If you want to know more about interceptor, please refer to [Interceptor](interceptor.md) for detail.

```go
// Invoke sends the RPC request on the wire and returns after response is                                                                              
// received.  This is typically called by generated code. 
//                                                                                                           
// All errors returned by Invoke are compatible with the status package.                                 
func (cc *ClientConn) Invoke(ctx context.Context, method string, args, reply interface{}, opts ...CallOption) error {
    // allow interceptor to see all applicable call options, which means those             
    // configured as defaults from dial option as well as per-call options
    opts = combine(cc.dopts.callOptions, opts)                                                                         
                                                                                
    if cc.dopts.unaryInt != nil {                                                      
        return cc.dopts.unaryInt(ctx, method, args, reply, cc, invoke, opts...)             
    }                  
    return invoke(ctx, method, args, reply, cc, opts...)             
}                                                                              
```

## Fork road

- `invoke()` creates a client stream `ClientStream` first. To send the request, we need a HTTP 2 stream.
  - `invoke()` calls `newClientStream()` to create a stream. Besides the stream, `newClientStream()` also sends the `Request-Headers` part.
- Then calls the `cs.SendMsg()` to send the request.
  - `invoke()` calls `cs.SendMsg()`, `cs.SendMsg()` is actually `clientStream.SendMsg()`.
  - `clientStream.SendMsg(req)` sends the `Length-Prefixed-Message` part,
  - `EOS` is just the flag set in the last data frame.
- Finally calls `cs.RecvMsg()` to receive the gRPC response.
  - `cs.RecvMsg()` is actually `clientStream.RecvMsg()`.
  - In this chapter, we will only touch the surface of `cs.RecvMsg()`. Our focus is the send request.
  - Please refer to [Send Response](docs/response.md) for more detail.

Next, lets' discuss `newClientStream()` first.

```go
func invoke(ctx context.Context, method string, req, reply interface{}, cc *ClientConn, opts ...CallOption) error {
    cs, err := newClientStream(ctx, unaryStreamDesc, cc, method, opts...)
    if err != nil {      
        return err      
    }                                                                                                                                     
    if err := cs.SendMsg(req); err != nil {      
        return err                       
    }                                              
    return cs.RecvMsg(reply)      
}      
```

## Send Request-Headers

`newClientStream()` creates the `ClientStream`, pick up a connection to the target server and send the `Request-Headers`.

- `newClientStream()` calls `cc.safeConfigSelector.SelectConfig()` to get the `RPCConfig`.
  - `cc.safeConfigSelector.SelectConfig()` is actually `SafeConfigSelector.SelectConfig()`.
- `newClientStream()` creates `clientStream`, which is the stream representation in gRPC.
- `newClientStream()` calls `cs.newAttemptLocked()` to create the `csAttempt` and pickup the connection to thte target server.
- `cs.newAttemptLocked()` creates a `csAttempt` and assign it to `cs.attempt`.
  - `cs.newAttemptLocked()` is `clientStream.newAttemptLocked()`.
  - `clientStream.newAttemptLocked()` creates a new attempt with a transport.
- `cs.withRetry()` is a mechanism to perform the "attempt" action with the predefined retry policy.
  - `cs.withRetry()` is `clientStream.withRetry()`.
- `op` is a anonymous wrapper function for the `a.newStream()` method,
  - `a` is the `csAttempt` just created with `cs.newAttemptLocked()`.
  - `a.newStream()` is `csAttempt.newStream()`.
  - `csAttempt.newStream()` create the client transport stream and use the stream to send the request header frame.

Next, let's discuss the `clientStream.newAttemptLocked()` first.

```go
func newClientStream(ctx context.Context, desc *StreamDesc, cc *ClientConn, method string, opts ...CallOption) (_ ClientStream, err error) {
    if channelz.IsOn() {
        cc.incrCallsStarted()
        defer func() {
            if err != nil {
                cc.incrCallsFailed()
            }
        }()
    }
    c := defaultCallInfo()
    // Provide an opportunity for the first RPC to see the first service config
    // provided by the resolver.
    if err := cc.waitForResolvedAddrs(ctx); err != nil {
        return nil, err
    }

    var mc serviceconfig.MethodConfig
    var onCommit func()
    rpcConfig, err := cc.safeConfigSelector.SelectConfig(iresolver.RPCInfo{Context: ctx, Method: method})
    if err != nil {
        return nil, status.Convert(err).Err()
    }
    if rpcConfig != nil {
        if rpcConfig.Context != nil {
            ctx = rpcConfig.Context
        }
        mc = rpcConfig.MethodConfig
        onCommit = rpcConfig.OnCommitted
    }

    if mc.WaitForReady != nil {
        c.failFast = !*mc.WaitForReady
    }

    // Possible context leak:
    // The cancel function for the child context we create will only be called
    // when RecvMsg returns a non-nil error, if the ClientConn is closed, or if
    // an error is generated by SendMsg.
    // https://github.com/grpc/grpc-go/issues/1818.
    var cancel context.CancelFunc
    if mc.Timeout != nil && *mc.Timeout >= 0 {
        ctx, cancel = context.WithTimeout(ctx, *mc.Timeout)
    } else {
        ctx, cancel = context.WithCancel(ctx)
    }
    defer func() {
        if err != nil {
            cancel()
        }
    }()

    for _, o := range opts {
        if err := o.before(c); err != nil {
            return nil, toRPCErr(err)
        }
    }
    c.maxSendMessageSize = getMaxSize(mc.MaxReqSize, c.maxSendMessageSize, defaultClientMaxSendMessageSize)
    c.maxReceiveMessageSize = getMaxSize(mc.MaxRespSize, c.maxReceiveMessageSize, defaultClientMaxReceiveMessageSize)
    if err := setCallInfoCodec(c); err != nil {
        return nil, err
    }

    callHdr := &transport.CallHdr{
        Host:           cc.authority,
        Method:         method,
        ContentSubtype: c.contentSubtype,
    }

    // Set our outgoing compression according to the UseCompressor CallOption, if
    // set.  In that case, also find the compressor from the encoding package.
    // Otherwise, use the compressor configured by the WithCompressor DialOption,
    // if set.
    var cp Compressor
    var comp encoding.Compressor
    if ct := c.compressorType; ct != "" {
        callHdr.SendCompress = ct
        if ct != encoding.Identity {
            comp = encoding.GetCompressor(ct)
            if comp == nil {
                return nil, status.Errorf(codes.Internal, "grpc: Compressor is not installed for requested grpc-encoding %q", ct)
            }
        }
    } else if cc.dopts.cp != nil {
        callHdr.SendCompress = cc.dopts.cp.Type()
        cp = cc.dopts.cp
    }
    if c.creds != nil {
        callHdr.Creds = c.creds
    }
    var trInfo *traceInfo
    if EnableTracing {
        trInfo = &traceInfo{
            tr: trace.New("grpc.Sent."+methodFamily(method), method),
            firstLine: firstLine{
                client: true,
            },
        }
        if deadline, ok := ctx.Deadline(); ok {
            trInfo.firstLine.deadline = time.Until(deadline)
        }
        trInfo.tr.LazyLog(&trInfo.firstLine, false)
        ctx = trace.NewContext(ctx, trInfo.tr)
    }
    ctx = newContextWithRPCInfo(ctx, c.failFast, c.codec, cp, comp)
    sh := cc.dopts.copts.StatsHandler
    var beginTime time.Time
    if sh != nil {
        ctx = sh.TagRPC(ctx, &stats.RPCTagInfo{FullMethodName: method, FailFast: c.failFast})
        beginTime = time.Now()
        begin := &stats.Begin{
            Client:    true,
            BeginTime: beginTime,
            FailFast:  c.failFast,
        }
        sh.HandleRPC(ctx, begin)
    }

    cs := &clientStream{
        callHdr:      callHdr,
        ctx:          ctx,
        methodConfig: &mc,
        opts:         opts,
        callInfo:     c,
        cc:           cc,
        desc:         desc,
        codec:        c.codec,
        cp:           cp,
        comp:         comp,
        cancel:       cancel,
        beginTime:    beginTime,
        firstAttempt: true,
        onCommit:     onCommit,
    }
    if !cc.dopts.disableRetry {
        cs.retryThrottler = cc.retryThrottler.Load().(*retryThrottler)
    }
    cs.binlog = binarylog.GetMethodLogger(method)

    // Only this initial attempt has stats/tracing.
    // TODO(dfawley): move to newAttempt when per-attempt stats are implemented.
    if err := cs.newAttemptLocked(sh, trInfo); err != nil {
        cs.finish(err)
        return nil, err
    }

    op := func(a *csAttempt) error { return a.newStream() }
    if err := cs.withRetry(op, func() { cs.bufferForRetryLocked(0, op) }); err != nil {
        cs.finish(err)
        return nil, err
    }

    if cs.binlog != nil {
        md, _ := metadata.FromOutgoingContext(ctx)
        logEntry := &binarylog.ClientHeader{
            OnClientSide: true,
            Header:       md,
            MethodName:   method,
            Authority:    cs.cc.authority,
        }
        if deadline, ok := ctx.Deadline(); ok {
            logEntry.Timeout = time.Until(deadline)
            if logEntry.Timeout < 0 {
                logEntry.Timeout = 0
            }
        }
        cs.binlog.Log(logEntry)
    }

    if desc != unaryStreamDesc {
        // Listen on cc and stream contexts to cleanup when the user closes the
        // ClientConn or cancels the stream context.  In all other cases, an error
        // should already be injected into the recv buffer by the transport, which
        // the client will eventually receive, and then we will cancel the stream's
        // context in clientStream.finish.
        go func() {
            select {
            case <-cc.ctx.Done():
                cs.finish(ErrClientConnClosing)
            case <-ctx.Done():
                cs.finish(toRPCErr(ctx.Err()))
            }
        }()
    }
    return cs, nil
}

// SelectConfig defers to the current ConfigSelector in scs.
func (scs *SafeConfigSelector) SelectConfig(r RPCInfo) (*RPCConfig, error) {
    scs.mu.RLock()
    defer scs.mu.RUnlock()
    return scs.cs.SelectConfig(r)     
}                                   
```

### Pick stream transport

In `newAttemptLocked()`,

- `newAttemptLocked()` creates `newAttempt`, which is a new attempt with a transport.
- `newAttemptLocked()` calls `cs.cc.getTransport()`, which is actually `ClientConn.getTransport()`.
- `ClientConn.getTransport()` calls `cc.blockingpicker.pick()`, which is actually `pickerWrapper.pick()`.
  - `pickerWrapper.pick()` returns the transport that will be used for the RPC.
  - `pickerWrapper.pick()` calls `p.Pick()` to return the connection to use for this RPC and related information.
    - `p := pw.picker`, `pw.picker` can be updated by method `pickerWrapper.updatePicker()`.
    - `ccBalancerWrapper.UpdateState()` will call `pickerWrapper.updatePicker()` to update it.
  - `pickerWrapper.pick()` calls `acw.getAddrConn().getReadyTransport()` to get the transport connection.
    - `acw.getAddrConn().getReadyTransport()` is actually `addrConn.getReadyTransport()`, which returns the transport if ac's state is READY.

Next, let's discuss the `clientStream.withRetry()` in detail.

```go
// newAttemptLocked creates a new attempt with a transport.
// If it succeeds, then it replaces clientStream's attempt with this new attempt.
func (cs *clientStream) newAttemptLocked(sh stats.Handler, trInfo *traceInfo) (retErr error) {
    newAttempt := &csAttempt{
        cs:           cs,
        dc:           cs.cc.dopts.dc,
        statsHandler: sh,
        trInfo:       trInfo,
    }
    defer func() {
        if retErr != nil {
            // This attempt is not set in the clientStream, so it's finish won't
            // be called. Call it here for stats and trace in case they are not
            // nil.
            newAttempt.finish(retErr)
        }
    }()

    if err := cs.ctx.Err(); err != nil {
        return toRPCErr(err)
    }

    ctx := cs.ctx
    if cs.cc.parsedTarget.Scheme == "xds" {
        // Add extra metadata (metadata that will be added by transport) to context
        // so the balancer can see them.
        ctx = grpcutil.WithExtraMetadata(cs.ctx, metadata.Pairs(
            "content-type", grpcutil.ContentType(cs.callHdr.ContentSubtype),
        ))
    }
    t, done, err := cs.cc.getTransport(ctx, cs.callInfo.failFast, cs.callHdr.Method)
    if err != nil {
        return err
    }
    if trInfo != nil {
        trInfo.firstLine.SetRemoteAddr(t.RemoteAddr())
    }
    newAttempt.t = t
    newAttempt.done = done
    cs.attempt = newAttempt
    return nil
}

func (cc *ClientConn) getTransport(ctx context.Context, failfast bool, method string) (transport.ClientTransport, func(balancer.DoneInfo), error) {
    t, done, err := cc.blockingpicker.pick(ctx, failfast, balancer.PickInfo{
        Ctx:            ctx,
        FullMethodName: method,
    })
    if err != nil {
        return nil, nil, toRPCErr(err)
    }
    return t, done, nil
}

// pick returns the transport that will be used for the RPC.
// It may block in the following cases:
// - there's no picker
// - the current picker returns ErrNoSubConnAvailable
// - the current picker returns other errors and failfast is false.
// - the subConn returned by the current picker is not READY
// When one of these situations happens, pick blocks until the picker gets updated.
func (pw *pickerWrapper) pick(ctx context.Context, failfast bool, info balancer.PickInfo) (transport.ClientTransport, func(balancer.DoneInfo), error) {
    var ch chan struct{}

    var lastPickErr error
    for {
        pw.mu.Lock()
        if pw.done {
            pw.mu.Unlock()
            return nil, nil, ErrClientConnClosing
        }

        if pw.picker == nil {
            ch = pw.blockingCh
        }
        if ch == pw.blockingCh {
            // This could happen when either:
            // - pw.picker is nil (the previous if condition), or
            // - has called pick on the current picker.
            pw.mu.Unlock()
            select {
            case <-ctx.Done():
                var errStr string
                if lastPickErr != nil {
                    errStr = "latest balancer error: " + lastPickErr.Error()
                } else {
                    errStr = ctx.Err().Error()
                }
                switch ctx.Err() {
                case context.DeadlineExceeded:
                    return nil, nil, status.Error(codes.DeadlineExceeded, errStr)
                case context.Canceled:
                    return nil, nil, status.Error(codes.Canceled, errStr)
                }
            case <-ch:
            }
            continue
        }

        ch = pw.blockingCh
        p := pw.picker
        pw.mu.Unlock()

        pickResult, err := p.Pick(info)

        if err != nil {
            if err == balancer.ErrNoSubConnAvailable {
                continue
            }
            if _, ok := status.FromError(err); ok {
                // Status error: end the RPC unconditionally with this status.
                return nil, nil, err
            }
            // For all other errors, wait for ready RPCs should block and other
            // RPCs should fail with unavailable.
            if !failfast {
                lastPickErr = err
                continue
            }
            return nil, nil, status.Error(codes.Unavailable, err.Error())
        }

        acw, ok := pickResult.SubConn.(*acBalancerWrapper)
        if !ok {
            logger.Error("subconn returned from pick is not *acBalancerWrapper")
            continue
        }
        if t, ok := acw.getAddrConn().getReadyTransport(); ok {
            if channelz.IsOn() {
                return t, doneChannelzWrapper(acw, pickResult.Done), nil
            }
            return t, pickResult.Done, nil
        }
        if pickResult.Done != nil {
            // Calling done with nil error, no bytes sent and no bytes received.
            // DoneInfo with default value works.
            pickResult.Done(balancer.DoneInfo{})
        }
        logger.Infof("blockingPicker: the picked transport is not ready, loop back to repick")
        // If ok == false, ac.state is not READY.
        // A valid picker always returns READY subConn. This means the state of ac
        // just changed, and picker will be updated shortly.
        // continue back to the beginning of the for loop to repick.
    }
}

// pickerWrapper is a wrapper of balancer.Picker. It blocks on certain pick
// actions and unblock when there's a picker update.
type pickerWrapper struct {
    mu         sync.Mutex
    done       bool
    blockingCh chan struct{}
    picker     balancer.Picker
}

func newPickerWrapper() *pickerWrapper {
    return &pickerWrapper{blockingCh: make(chan struct{})}
}

// updatePicker is called by UpdateBalancerState. It unblocks all blocked pick.
func (pw *pickerWrapper) updatePicker(p balancer.Picker) {
    pw.mu.Lock()
    if pw.done {
        pw.mu.Unlock()
        return
    }
    pw.picker = p
    // pw.blockingCh should never be nil.
    close(pw.blockingCh)
    pw.blockingCh = make(chan struct{})
    pw.mu.Unlock()
}

// Picker is used by gRPC to pick a SubConn to send an RPC.
// Balancer is expected to generate a new picker from its snapshot every time its
// internal state has changed.
//
// The pickers used by gRPC can be updated by ClientConn.UpdateState().
type Picker interface {
    // Pick returns the connection to use for this RPC and related information.
    //
    // Pick should not block.  If the balancer needs to do I/O or any blocking
    // or time-consuming work to service this call, it should return
    // ErrNoSubConnAvailable, and the Pick call will be repeated by gRPC when
    // the Picker is updated (using ClientConn.UpdateState).
    //
    // If an error is returned:
    //
    // - If the error is ErrNoSubConnAvailable, gRPC will block until a new
    //   Picker is provided by the balancer (using ClientConn.UpdateState).
    //
    // - If the error is a status error (implemented by the grpc/status
    //   package), gRPC will terminate the RPC with the code and message
    //   provided.
    //
    // - For all other errors, wait for ready RPCs will wait, but non-wait for
    //   ready RPCs will be terminated with this error's Error() string and
    //   status code Unavailable.
    Pick(info PickInfo) (PickResult, error)
}

// getReadyTransport returns the transport if ac's state is READY.
// Otherwise it returns nil, false.
// If ac's state is IDLE, it will trigger ac to connect.
func (ac *addrConn) getReadyTransport() (transport.ClientTransport, bool) {
    ac.mu.Lock()
    if ac.state == connectivity.Ready && ac.transport != nil {
        t := ac.transport
        ac.mu.Unlock()
        return t, true
    }
    var idle bool
    if ac.state == connectivity.Idle {
        idle = true
    }
    ac.mu.Unlock()
    // Trigger idle ac to connect.
    if idle {
        ac.connect()
    }
    return nil, false
}
```

### Send header

Now, the stream transport is ready. It's time to send the RPC header.

`clientStream.withRetry()` is a mechanism to perform the "attempt" action with the predefined retry policy.

- `clientStream.withRetry()` retries the `op` function in the for loop.
- `op` is a function with the `csAttempt` as only parameter.
- if `op` is called successfully, the `onSuccess` function will be called.
- Otherwise, `clientStream.withRetry()` calls `cs.retryLocked()` to check whether retry is a good choice.

Retry policy deserves a dedicated chapter. Here we will focus on our main path. See [Here] for detail.

`withRetry()` need `op` function to perform the "attempt" action. Next, let's discuss the `csAttempt.newStream()` action in detail.

```go
func (cs *clientStream) withRetry(op func(a *csAttempt) error, onSuccess func()) error {
    cs.mu.Lock()
    for {
        if cs.committed {
            cs.mu.Unlock()
            return op(cs.attempt)
        }
        a := cs.attempt
        cs.mu.Unlock()
        err := op(a)
        cs.mu.Lock()
        if a != cs.attempt {
            // We started another attempt already.
            continue
        }
        if err == io.EOF {
            <-a.s.Done()
        }
        if err == nil || (err == io.EOF && a.s.Status().Code() == codes.OK) {
            onSuccess()
            cs.mu.Unlock()
            return err
        }
        if err := cs.retryLocked(err); err != nil {
            cs.mu.Unlock()
            return err
        }
    }
}

// Returns nil if a retry was performed and succeeded; error otherwise.
func (cs *clientStream) retryLocked(lastErr error) error {
    for {
        cs.attempt.finish(lastErr)
        if err := cs.shouldRetry(lastErr); err != nil {
            cs.commitAttemptLocked()
            return err
        }
        cs.firstAttempt = false
        if err := cs.newAttemptLocked(nil, nil); err != nil {
            return err
        }
        if lastErr = cs.replayBufferLocked(); lastErr == nil {
            return nil
        }
    }
}

```

In `newClientStream()`, The `op` parameter for the `cs.withRetry()` is

```go
op := func(a *csAttempt) error { return a.newStream() }
```

Which means `op` is an anonymous wrapper function for the `a.newStream()`, `a.newStream()` is `csAttempt.newStream()`. In `csAttempt.newStream()`,

- `a.t.NewStream()` is called to create the header frame and send the header frame to server.
- `a.t.NewStream()` is actually `http2Client.NewStream()`,
  - In `http2Client.NewStream()`.
    - `t.createHeaderFields()` is called to build the HPACK header fields, `t.createHeaderFields()` is actually `http2Client.createHeaderFields()`.
    - `t.newStream()` is called to create the client transport stream, `t.newStream()` is actually `http2Client.newStream()`.
    - `http2Client.NewStream()` builds the `headerFrame` and initialize its fields.
    - `http2Client.NewStream()` calls `t.controlBuf.executeAndPut()` to send the header.
  - Now the `Request-Headers` has been sent to target server.
- if `http2Client.NewStream()` successes, `csAttempt.newStream()` saves the stream to `cs.attempt.s` and assigns a new `parser` to `cs.attempt.p`.

Here only show the code snippet of `NewStream()` and `createHeaderFields()`.

In the `http2Client.createHeaderFields()`,

- the `:path` header field has value `"/helloworld.Greeter/SayHello"`.
- please also note the value of  `:authority` header field and `content-type` header field.

`t.controlBuf` deserve another chapter, see [controlBuffer, loopyWriter and framer](control.md) for detail.

```go
func (a *csAttempt) newStream() error {
    cs := a.cs
    cs.callHdr.PreviousAttempts = cs.numRetries
    s, err := a.t.NewStream(cs.ctx, cs.callHdr)
    if err != nil {
        if _, ok := err.(transport.PerformedIOError); ok {
            // Return without converting to an RPC error so retry code can
            // inspect.
            return err
        }
        return toRPCErr(err)
    }
    cs.attempt.s = s
    cs.attempt.p = &parser{r: s}
    return nil
}

// NewStream creates a stream and registers it into the transport as "active"
// streams.
func (t *http2Client) NewStream(ctx context.Context, callHdr *CallHdr) (_ *Stream, err error) {
    ctx = peer.NewContext(ctx, t.getPeer())
    headerFields, err := t.createHeaderFields(ctx, callHdr)
    if err != nil {
        // We may have performed I/O in the per-RPC creds callback, so do not
        // allow transparent retry.
        return nil, PerformedIOError{err}
    }
    s := t.newStream(ctx, callHdr)
    cleanup := func(err error) {
+-- 12 lines: if s.swapState(streamDone) == streamDone {·······································································································
    }
    hdr := &headerFrame{
        hf:        headerFields,
        endStream: false,
        initStream: func(id uint32) error {
            t.mu.Lock()
            if state := t.state; state != reachable {
                t.mu.Unlock()
                // Do a quick cleanup.
                err := error(errStreamDrain)
                if state == closing {
                    err = ErrConnClosing
                }
                cleanup(err)
                return err
            }
            t.activeStreams[id] = s
            if channelz.IsOn() {
                atomic.AddInt64(&t.czData.streamsStarted, 1)
                atomic.StoreInt64(&t.czData.lastStreamCreatedTime, time.Now().UnixNano())
            }
            // If the keepalive goroutine has gone dormant, wake it up.
            if t.kpDormant {
                t.kpDormancyCond.Signal()
            }
            t.mu.Unlock()
            return nil
        },
        onOrphaned: cleanup,
        wq:         s.wq,
    }
    firstTry := true
    var ch chan struct{}
    checkForStreamQuota := func(it interface{}) bool {
+-- 23 lines: if t.streamQuota <= 0 { Can go negative if server decreases it.··················································································
    }
    var hdrListSizeErr error
    checkForHeaderListSize := func(it interface{}) bool {
+-- 12 lines: if t.maxSendHeaderListSize == nil {··············································································································
    }
    for {
        success, err := t.controlBuf.executeAndPut(func(it interface{}) bool {
            if !checkForStreamQuota(it) {
                return false
            }
            if !checkForHeaderListSize(it) {
                return false
            }
            return true
        }, hdr)
        if err != nil {
            return nil, err
        }
        if success {
            break
        }
        if hdrListSizeErr != nil {
            return nil, hdrListSizeErr
        }
        firstTry = false
        select {
        case <-ch:
        case <-s.ctx.Done():
            return nil, ContextErr(s.ctx.Err())
        case <-t.goAway:
            return nil, errStreamDrain
        case <-t.ctx.Done():
            return nil, ErrConnClosing
        }
    }
    if t.statsHandler != nil {
+-- 17 lines: header, ok := metadata.FromOutgoingContext(ctx)··································································································
    }
    return s, nil
}

func (t *http2Client) createHeaderFields(ctx context.Context, callHdr *CallHdr) ([]hpack.HeaderField, error) {
    aud := t.createAudience(callHdr)
    ri := credentials.RequestInfo{
        Method:   callHdr.Method,
        AuthInfo: t.authInfo,
    }
    ctxWithRequestInfo := internal.NewRequestInfoContext.(func(context.Context, credentials.RequestInfo) context.Context)(ctx, ri)
    authData, err := t.getTrAuthData(ctxWithRequestInfo, aud)
    if err != nil {
        return nil, err
    }
    callAuthData, err := t.getCallAuthData(ctxWithRequestInfo, aud, callHdr)
    if err != nil {
        return nil, err
    }
    // TODO(mmukhi): Benchmark if the performance gets better if count the metadata and other header fields
    // first and create a slice of that exact size.
    // Make the slice of certain predictable size to reduce allocations made by append.
    hfLen := 7 // :method, :scheme, :path, :authority, content-type, user-agent, te
    hfLen += len(authData) + len(callAuthData)
    headerFields := make([]hpack.HeaderField, 0, hfLen)
    headerFields = append(headerFields, hpack.HeaderField{Name: ":method", Value: "POST"})
    headerFields = append(headerFields, hpack.HeaderField{Name: ":scheme", Value: t.scheme})
    headerFields = append(headerFields, hpack.HeaderField{Name: ":path", Value: callHdr.Method})
    headerFields = append(headerFields, hpack.HeaderField{Name: ":authority", Value: callHdr.Host})
    headerFields = append(headerFields, hpack.HeaderField{Name: "content-type", Value: grpcutil.ContentType(callHdr.ContentSubtype)})
    headerFields = append(headerFields, hpack.HeaderField{Name: "user-agent", Value: t.userAgent})
    headerFields = append(headerFields, hpack.HeaderField{Name: "te", Value: "trailers"})
    if callHdr.PreviousAttempts > 0 {
        headerFields = append(headerFields, hpack.HeaderField{Name: "grpc-previous-rpc-attempts", Value: strconv.Itoa(callHdr.PreviousAttempts)})
    }

    if callHdr.SendCompress != "" {
        headerFields = append(headerFields, hpack.HeaderField{Name: "grpc-encoding", Value: callHdr.SendCompress})
        headerFields = append(headerFields, hpack.HeaderField{Name: "grpc-accept-encoding", Value: callHdr.SendCompress})
    }
    if dl, ok := ctx.Deadline(); ok {
        // Send out timeout regardless its value. The server can detect timeout context by itself.
        // TODO(mmukhi): Perhaps this field should be updated when actually writing out to the wire.
        timeout := time.Until(dl)
        headerFields = append(headerFields, hpack.HeaderField{Name: "grpc-timeout", Value: grpcutil.EncodeDuration(timeout)})
    }
    for k, v := range authData {
        headerFields = append(headerFields, hpack.HeaderField{Name: k, Value: encodeMetadataHeader(k, v)})
    }
    for k, v := range callAuthData {
        headerFields = append(headerFields, hpack.HeaderField{Name: k, Value: encodeMetadataHeader(k, v)})
    }
    if b := stats.OutgoingTags(ctx); b != nil {
        headerFields = append(headerFields, hpack.HeaderField{Name: "grpc-tags-bin", Value: encodeBinHeader(b)})
    }
    if b := stats.OutgoingTrace(ctx); b != nil {
        headerFields = append(headerFields, hpack.HeaderField{Name: "grpc-trace-bin", Value: encodeBinHeader(b)})
    }

    if md, added, ok := metadata.FromOutgoingContextRaw(ctx); ok {
+-- 23 lines: var k string············································································································································
    }
    for k, vv := range t.md {
+--  6 lines: if isReservedHeader(k) {································································································································
    }
    return headerFields, nil
}
```

## Send Length-Prefixed-Message and EOS

Now, we picked a connection to target server, created a stream and sent the request header. It's time to back to the [Fork road](#fork-road) and check the `cs.SendMsg()`, which is actually `clientStream.SendMsg()`.

- `clientStrea.SendMsg()` calls  `prepareMsg()` to encode and compress (if required) the gRPC call request parameter to byte slice.
  - Here, we will not go deeper into the `prepareMsg()`. You need more HTTP 2 knowledge to understand it.
  - The result of `prepareMsg()` is encoding / compressing the request parameter into frame header and frame payload slice.
- Here you can see the `op` function and `cs.withRetry()` structure again.
  - `cs.withRetry()` is a mechanism to perform the "attempt" action with the predefined retry policy.
  - `op` is the "attempt" action.
  - `op` is a anonymous wrapper function for the `a.sendMsg()` method, where `a` is the `csAttempt` created before,
    - `a.sendMsg()` is called to send the gRPC call request parameter.
    - `a.sendMsg()` is actually `csAttempt.sendMsg()`.

In `csAttempt.sendMsg()`,

- `csAttempt.sendMsg()` calls `a.t.Write()` to send the gRPC call request parameter.
  - Here `a.t.Write()` is actually `http2Client.Write()`.
    - `http2Client.Write()` builds the `dataFrame` and initialize its fields,
    - `http2Client.Write()` calls `t.controlBuf.put()` to send the request parameter.
  - Now the `Length-Prefixed-Message` has been sent
- Please note in `&transport.Options{Last: !cs.desc.ClientStreams}`, the `Last` field is true.
- Which means this is the last frame for this request: the `EOS` flag is set now.

`t.controlBuf` deserves another chapter, see [controlBuffer, loopyWriter and framer](control.md) for detail.

```go
func (cs *clientStream) SendMsg(m interface{}) (err error) {
    defer func() {
        if err != nil && err != io.EOF {
            // Call finish on the client stream for errors generated by this SendMsg
            // call, as these indicate problems created by this client.  (Transport
            // errors are converted to an io.EOF error in csAttempt.sendMsg; the real
            // error will be returned from RecvMsg eventually in that case, or be
            // retried.)
            cs.finish(err)
        }
    }()
    if cs.sentLast {
        return status.Errorf(codes.Internal, "SendMsg called after CloseSend")
    }
    if !cs.desc.ClientStreams {
        cs.sentLast = true
    }

    // load hdr, payload, data
    hdr, payload, data, err := prepareMsg(m, cs.codec, cs.cp, cs.comp)
    if err != nil {
        return err
    }

    // TODO(dfawley): should we be checking len(data) instead?
    if len(payload) > *cs.callInfo.maxSendMessageSize {
        return status.Errorf(codes.ResourceExhausted, "trying to send message larger than max (%d vs. %d)", len(payload), *cs.callInfo.maxSendMessageSize)
    }
    msgBytes := data // Store the pointer before setting to nil. For binary logging.
    op := func(a *csAttempt) error {
        err := a.sendMsg(m, hdr, payload, data)
        // nil out the message and uncomp when replaying; they are only needed for
        // stats which is disabled for subsequent attempts.
        m, data = nil, nil
        return err
    }
    err = cs.withRetry(op, func() { cs.bufferForRetryLocked(len(hdr)+len(payload), op) })
    if cs.binlog != nil && err == nil {
        cs.binlog.Log(&binarylog.ClientMessage{
            OnClientSide: true,
            Message:      msgBytes,
        })
    }
    return
}

// prepareMsg returns the hdr, payload and data
// using the compressors passed or using the
// passed preparedmsg
func prepareMsg(m interface{}, codec baseCodec, cp Compressor, comp encoding.Compressor) (hdr, payload, data []byte, err error) {
    if preparedMsg, ok := m.(*PreparedMsg); ok {
        return preparedMsg.hdr, preparedMsg.payload, preparedMsg.encodedData, nil
    }
    // The input interface is not a prepared msg.
    // Marshal and Compress the data at this point
    data, err = encode(codec, m)
    if err != nil {
        return nil, nil, nil, err
    }
    compData, err := compress(data, cp, comp)
    if err != nil {
        return nil, nil, nil, err
    }
    hdr, payload = msgHeader(data, compData)
    return hdr, payload, data, nil
}

func (a *csAttempt) sendMsg(m interface{}, hdr, payld, data []byte) error {
    cs := a.cs
    if a.trInfo != nil {
        a.mu.Lock()
        if a.trInfo.tr != nil {
            a.trInfo.tr.LazyLog(&payload{sent: true, msg: m}, true)
        }
        a.mu.Unlock()
    }
    if err := a.t.Write(a.s, hdr, payld, &transport.Options{Last: !cs.desc.ClientStreams}); err != nil {
        if !cs.desc.ClientStreams {
            // For non-client-streaming RPCs, we return nil instead of EOF on error
            // because the generated code requires it.  finish is not called; RecvMsg()
            // will call it with the stream's status independently.
            return nil
        }
        return io.EOF
    }
    if a.statsHandler != nil {
        a.statsHandler.HandleRPC(cs.ctx, outPayload(true, m, data, payld, time.Now()))
    }
    if channelz.IsOn() {
        a.t.IncrMsgSent()
    }
    return nil
}

// Write formats the data into HTTP2 data frame(s) and sends it out. The caller
// should proceed only if Write returns nil.
func (t *http2Client) Write(s *Stream, hdr []byte, data []byte, opts *Options) error {
    if opts.Last {
        // If it's the last message, update stream state.
        if !s.compareAndSwapState(streamActive, streamWriteDone) {
            return errStreamDone
        }
    } else if s.getState() != streamActive {
        return errStreamDone
    }
    df := &dataFrame{
        streamID:  s.id,
        endStream: opts.Last,
        h:         hdr,
        d:         data,
    }
    if hdr != nil || data != nil { // If it's not an empty data frame, check quota.
        if err := s.wq.get(int32(len(hdr) + len(data))); err != nil {
            return err
        }
    }
    return t.controlBuf.put(df)
}
```

## Receive response

Now the request has been sent. It's time to back to the Fork road and check the `cs.RecvMsg()`, which is actually `clientStream.RecvMsg()`.

- `clientStream.RecvMsg()` calls `a.recvMsg()` to receive `recvInfo` and convert it into response type.
- Here you can see the `op` function and `cs.withRetry()` structure again.
  - `cs.withRetry()` is a mechanism to perform the "attempt" action with the predefined retry policy.
  - `op` is the "attempt" action.
  - There is an anonymous wrapper function for the `a.recvMsg()` method, where `a` is the `csAttempt`.
    - `a.recvMsg()` is called to receive the gRPC response.
    - `a.recvMsg()` is actually `csAttempt.recvMsg()`.

In `csAttempt.recvMsg()`,

- `csAttempt.recvMsg()` calls `recv()` to receive the gRPC response payload and convert it into response type.
- `recv()` return `io.EOF` to indicates successful end of stream.
- Here, we will not go deeper into the `recv()` . You need more HTTP 2 knowledge to understand it.

```go
func (cs *clientStream) RecvMsg(m interface{}) error {
    if cs.binlog != nil && !cs.serverHeaderBinlogged {
        // Call Header() to binary log header if it's not already logged.
        cs.Header()
    }
    var recvInfo *payloadInfo
    if cs.binlog != nil {
        recvInfo = &payloadInfo{}
    }
    err := cs.withRetry(func(a *csAttempt) error {
        return a.recvMsg(m, recvInfo)
    }, cs.commitAttemptLocked)
    if cs.binlog != nil && err == nil {
        cs.binlog.Log(&binarylog.ServerMessage{
            OnClientSide: true,
            Message:      recvInfo.uncompressedBytes,
        })
    }
    if err != nil || !cs.desc.ServerStreams {
        // err != nil or non-server-streaming indicates end of stream.
        cs.finish(err)

        if cs.binlog != nil {
            // finish will not log Trailer. Log Trailer here.
            logEntry := &binarylog.ServerTrailer{
                OnClientSide: true,
                Trailer:      cs.Trailer(),
                Err:          err,
            }
            if logEntry.Err == io.EOF {
                logEntry.Err = nil
            }
            if peer, ok := peer.FromContext(cs.Context()); ok {
                logEntry.PeerAddr = peer.Addr
            }
            cs.binlog.Log(logEntry)
        }
    }
    return err
}

func (a *csAttempt) recvMsg(m interface{}, payInfo *payloadInfo) (err error) {
    cs := a.cs
    if a.statsHandler != nil && payInfo == nil {
        payInfo = &payloadInfo{}
    }

    if !a.decompSet {
        // Block until we receive headers containing received message encoding.
        if ct := a.s.RecvCompress(); ct != "" && ct != encoding.Identity {
            if a.dc == nil || a.dc.Type() != ct {
                // No configured decompressor, or it does not match the incoming
                // message encoding; attempt to find a registered compressor that does.
                a.dc = nil
                a.decomp = encoding.GetCompressor(ct)
            }
        } else {
            // No compression is used; disable our decompressor.
            a.dc = nil
        }
        // Only initialize this state once per stream.
        a.decompSet = true
    }
    err = recv(a.p, cs.codec, a.s, a.dc, m, *cs.callInfo.maxReceiveMessageSize, payInfo, a.decomp)
    if err != nil {
        if err == io.EOF {
            if statusErr := a.s.Status().Err(); statusErr != nil {
                return statusErr
            }
            return io.EOF // indicates successful end of stream.
        }
        return toRPCErr(err)
    }
    if a.trInfo != nil {
        a.mu.Lock()
        if a.trInfo.tr != nil {
            a.trInfo.tr.LazyLog(&payload{sent: false, msg: m}, true)
        }
        a.mu.Unlock()
    }
    if a.statsHandler != nil {
        a.statsHandler.HandleRPC(cs.ctx, &stats.InPayload{
            Client:   true,
            RecvTime: time.Now(),
            Payload:  m,
            // TODO truncate large payload.
            Data:       payInfo.uncompressedBytes,
            WireLength: payInfo.wireLength + headerLen,
            Length:     len(payInfo.uncompressedBytes),
        })
    }
    if channelz.IsOn() {
        a.t.IncrMsgRecv()
    }
    if cs.desc.ServerStreams {
        // Subsequent messages should be received by subsequent RecvMsg calls.
        return nil
    }
    // Special handling for non-server-stream rpcs.
    // This recv expects EOF or errors, so we don't collect inPayload.
    err = recv(a.p, cs.codec, a.s, a.dc, m, *cs.callInfo.maxReceiveMessageSize, nil, a.decomp)
    if err == nil {
        return toRPCErr(errors.New("grpc: client streaming protocol violation: get <nil>, want <EOF>"))
    }
    if err == io.EOF {
        return a.s.Status().Err() // non-server streaming Recv returns nil on success
    }
    return toRPCErr(err)
}

// For the two compressor parameters, both should not be set, but if they are,
// dc takes precedence over compressor.
// TODO(dfawley): wrap the old compressor/decompressor using the new API?
func recv(p *parser, c baseCodec, s *transport.Stream, dc Decompressor, m interface{}, maxReceiveMessageSize int, payInfo *payloadInfo, compressor encoding.Compressor) error {
    d, err := recvAndDecompress(p, s, dc, maxReceiveMessageSize, payInfo, compressor)
    if err != nil {
        return err
    }
    if err := c.Unmarshal(d, m); err != nil {
        return status.Errorf(codes.Internal, "grpc: failed to unmarshal the received message %v", err)
    }
    if payInfo != nil {
        payInfo.uncompressedBytes = d
    }
    return nil
}
```
