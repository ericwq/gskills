# Send Response

- [Application code](#application-code)
- [Register service](#register-service)
- [Serve request](#serve-request)
  - [Prepare for stream](#prepare-for-stream)
  - [Serve stream](#serve-stream)
  - [Read frame](#read-frame)
  - [Decode header](#decode-header)
  - [Server worker](#server-worker)
  - [Handle request](#handle-request)
  - [Process unary RPC](#process-unary-rpc)
  - [Receive method parameters](#receive-method-parameters)
  - [Invoke service method](#invoke-service-method)
  - [Send response header and data](#send-response-header-and-data)
  - [Finish stream](#finish-stream)

gRPC uses HTTP 2 frames to send request and process response. But how to do that exactly? Let's explain the detail of implementation of server side response. [gRPC over HTTP2](https://github.com/grpc/grpc/blob/master/doc/PROTOCOL-HTTP2.md) is a good start point to explain the design of gRPC over HTTP 2. In brief, the gRPC response is composited by three parts:

```txt
Response → (Response-Headers *Length-Prefixed-Message Trailers) / Trailers-Only
```

- A header frame (`Response-Headers`),
- Zero or more data frame (`Length-Prefixed-Message`),
- The final part is `Trailers`. A special kind of header frame which can be sent after the body. These headers allow for metadata that can’t be calculated up front. In special case, Trailers can be send alone.

Please refer to the [Send Request](request.md) to get more information about the request. After all we need to know the request before the reply.

## Application code

Here is the gRPC server side application code snippet.

- It uses `net.Listen("tcp", port)` to bind the server side listening port.
- Then it creates a gRPC server with `grpc.NewServer()` and register the implementation of `"helloworld.GreeterServer"` service on the gRPC server.
- At last, it uses `s.Serve(lis)`to serve the listening port.

It looks easy and straightforward, right? With the help from protocol buffer, the `server` implementation is easy. It just implement the `SayHello()` method defined by protocol buffer IDL.

Please note that the `server.SayHello()` will be the final receiver of gRPC request. Final receiver means it will not serve the client side request directly. This article will reveal the whole process to you. Let's start our discussion from `pb.RegisterGreeterServer()`. Which prepares the necessary setting before the gRPC call.

```go
const (                                
    port = ":50051"                                
)                                
                                
// server is used to implement helloworld.GreeterServer.                                
type server struct {                                
    pb.UnimplementedGreeterServer                                
}                                
                                
// SayHello implements helloworld.GreeterServer                                
func (s *server) SayHello(ctx context.Context, in *pb.HelloRequest) (*pb.HelloReply, error) {                                
    log.Printf("Received: %v", in.GetName())                                
    return &pb.HelloReply{Message: "Hello " + in.GetName()}, nil                                
}                                
                                
func main() {                                
    lis, err := net.Listen("tcp", port)                                
    if err != nil {                                
        log.Fatalf("failed to listen: %v", err)                                
    }                                
    s := grpc.NewServer()                                
    pb.RegisterGreeterServer(s, &server{})                                
    if err := s.Serve(lis); err != nil {                                
        log.Fatalf("failed to serve: %v", err)                                
    }                                
}                                
```

## Register service

The following code is generated by gRPC plug-in. `pb.RegisterGreeterServer()` registers the `GreeterServer` service in gRPC server. Here `s` is the gRPC server, `&server{}` is the service.

- `pb.RegisterGreeterServer()` uses `_Greeter_serviceDesc` to register the `GreeterServer` service.
- `_Greeter_serviceDesc` is of type `grpc.ServiceDesc`, which is a data structure to store the service related configuration.

In `_Greeter_serviceDesc`,

- There is a key function `_Greeter_SayHello_Handler()`, which is the so called `methodHandler()`.
- Please note the signature of `_Greeter_SayHello_Handler()` and `methodHandler()` is the same.
- `methodHandler` will be called  when the server ready to process the request.
- In `_Greeter_SayHello_Handler()`, `server.SayHello()` will be called to fulfill the request.
- That's way I say `server.SayHello()` is the final receiver .

Why we need `methodHandler()`? Because protocol buffer message need to be encoded/decoded and lots of features provided by gRPC need to get a chance to run.

- `_Greeter_SayHello_Handler()` use `dec(in)` to decode the incoming message and build the `HelloRequest` object.
- If there is no interceptor `_Greeter_SayHello_Handler()` calls `srv.(GreeterServer).SayHello()` to process the request.
- If any interceptor (chain) exist, `_Greeter_SayHello_Handler()` calls `interceptor()` to activate the interceptor chains.
  - `interceptor()` is of type `UnaryServerInterceptor`, which needs a `UnaryHandler` parameter.
  - `handler` is an anonymous wrapper function for `GreeterServer.SayHello()`. `handler` is the `UnaryHandler` parameter.

Please note the `srv.(GreeterServer).SayHello(ctx, in)` and `interceptor(ctx, in, info, handler)` share the same return type. With or without interceptor, `_Greeter_SayHello_Handler()` works as expected. With this design, the interceptor got a chance to run. See [Interceptor](interceptor.md) for more detail.

Next, let's discuss the `s.RegisterService()` in detail.

```go
func RegisterGreeterServer(s grpc.ServiceRegistrar, srv GreeterServer) {                                
    s.RegisterService(&_Greeter_serviceDesc, srv)                                
}                                
                                
func _Greeter_SayHello_Handler(srv interface{}, ctx context.Context, dec func(interface{}) error, interceptor grpc.UnaryServerInterceptor) (interface{}, error)       {                                                                
    in := new(HelloRequest)                                
    if err := dec(in); err != nil {                                
        return nil, err                                
    }                                
    if interceptor == nil {                                
        return srv.(GreeterServer).SayHello(ctx, in)                                
    }                                
    info := &grpc.UnaryServerInfo{                                
        Server:     srv,                                
        FullMethod: "/helloworld.Greeter/SayHello",                                
    }                                
    handler := func(ctx context.Context, req interface{}) (interface{}, error) {                                
        return srv.(GreeterServer).SayHello(ctx, req.(*HelloRequest))                                
    }                                
    return interceptor(ctx, in, info, handler)                                
}                                
                                
var _Greeter_serviceDesc = grpc.ServiceDesc{                                
    ServiceName: "helloworld.Greeter",                                
    HandlerType: (*GreeterServer)(nil),                                
    Methods: []grpc.MethodDesc{                                
        {                                
            MethodName: "SayHello",                                
            Handler:    _Greeter_SayHello_Handler,                                
        },                                
    },                                
    Streams:  []grpc.StreamDesc{},                                
    Metadata: "examples/helloworld/helloworld/helloworld.proto",                                
}  

type methodHandler func(srv interface{}, ctx context.Context, dec func(interface{}) error, interceptor UnaryServerInterceptor) (interface{}, error)

// UnaryHandler defines the handler invoked by UnaryServerInterceptor to complete the normal
// execution of a unary RPC. If a UnaryHandler returns an error, it should be produced by the
// status package, or else gRPC will use codes.Unknown as the status code and err.Error() as
// the status message of the RPC.
type UnaryHandler func(ctx context.Context, req interface{}) (interface{}, error)

// UnaryServerInterceptor provides a hook to intercept the execution of a unary RPC on the server. info
// contains all the information of this RPC the interceptor can operate on. And handler is the wrapper
// of the service method implementation. It is the responsibility of the interceptor to invoke handler
// to complete the RPC.
type UnaryServerInterceptor func(ctx context.Context, req interface{}, info *UnaryServerInfo, handler UnaryHandler) (resp interface{}, err error)
```

`RegisterService()` first make sure the `ss` implement the `HandlerType` of `ServiceDesc`. In our case, `HandlerType` is `*GreeterServer`. See `HandlerType` in `_Greeter_serviceDesc`.

`s.register()` register the service in the gRPC server by adding an item in `s.services` map. `s.register()` also convert the `ServiceDesc` into `serviceInfo`.  `s.services` is the place to store the registered service. New service is registered by the service name, method name and service implementation. When the client send a gRPC request, the client need to provide both the service name and method name in request. See [Send Request](request.md) for more detail.

In our case:

- Service name is `"helloworld.Greeter"`, method name is `"SayHello"`.
- In `Server` struct, the `services` is a map, its key is the service name, the value is the `serviceInfo`.

```go
// RegisterService registers a service and its implementation to the gRPC
// server. It is called from the IDL generated code. This must be called before
// invoking Serve. If ss is non-nil (for legacy code), its type is checked to
// ensure it implements sd.HandlerType.
func (s *Server) RegisterService(sd *ServiceDesc, ss interface{}) {
    if ss != nil {                                      
        ht := reflect.TypeOf(sd.HandlerType).Elem()                                                                    
        st := reflect.TypeOf(ss)           
        if !st.Implements(ht) {                                                                                                 
            logger.Fatalf("grpc: Server.RegisterService found the handler of type %v that does not satisfy %v", st, ht)
        }                     
    }
    s.register(sd, ss) 
}                                                           

func (s *Server) register(sd *ServiceDesc, ss interface{}) {
    s.mu.Lock()                                                                                                          
    defer s.mu.Unlock()                                                                                                     
    s.printf("RegisterService(%q)", sd.ServiceName)
    if s.serve {
        logger.Fatalf("grpc: Server.RegisterService after Server.Serve for %q", sd.ServiceName)
    }
    if _, ok := s.services[sd.ServiceName]; ok {
        logger.Fatalf("grpc: Server.RegisterService found duplicate service registration for %q", sd.ServiceName)
    }
    info := &serviceInfo{
        serviceImpl: ss,
        methods:     make(map[string]*MethodDesc),
        streams:     make(map[string]*StreamDesc),
        mdata:       sd.Metadata,
    }
    for i := range sd.Methods {
        d := &sd.Methods[i]
        info.methods[d.MethodName] = d
    }
    for i := range sd.Streams {
        d := &sd.Streams[i]
        info.streams[d.StreamName] = d
    }
    s.services[sd.ServiceName] = info
}

type Server struct {
    opts serverOptions                                                                         
     
    mu       sync.Mutex // guards following     
    lis      map[net.Listener]bool                                                                               
    conns    map[transport.ServerTransport]bool
    serve    bool        
    drain    bool       
    cv       *sync.Cond              // signaled when connections close for GracefulStop
    services map[string]*serviceInfo // service name -> service info
    events   trace.EventLog      
     
    quit               *grpcsync.Event
    done               *grpcsync.Event
    channelzRemoveOnce sync.Once      
    serveWG            sync.WaitGroup // counts active Serve goroutines for GracefulStop
                               
    channelzID int64 // channelz unique identification number
    czData     *channelzData          
     
    serverWorkerChannels []chan *serverWorkerData
}
```

## Serve request

The following is the serve request sequence diagram.  It focus on the three parts: `Response-Headers`, `Length-Prefixed-Message` and `Trailers`.

- Yellow box represents the important type and method/function.
- Blue box represents a placeholder for the runtime type.
- Arrow represents the call direction and order.
- Right red dot means there is another map for that box.

![images.004.png](../images/images.004.png)

Now everything is ready. Let's serve the gRPC request. `Serve()` is the entry point.

- `Serve()` waits and accepts the incoming request connection and
- `Serve()` calls `s.handleRawConn()` to start a new goroutine to process the incoming request.
- Please note `Serve()` need to be called in application code.

The following request will not be blocked because of the anonymous goroutine. Every new request will be served by a new goroutine. Please focus on the for loop because the code is a little bit long.

Next, let's discuss the `s.handleRawConn()` in detail.

```go
// Serve accepts incoming connections on the listener lis, creating a new
// ServerTransport and service goroutine for each. The service goroutines
// read gRPC requests and then call the registered handlers to reply to them.
// Serve returns when lis.Accept fails with fatal errors.  lis will be closed when
// this method returns.
// Serve will return a non-nil error unless Stop or GracefulStop is called.
func (s *Server) Serve(lis net.Listener) error {
    s.mu.Lock()
    s.printf("serving")
    s.serve = true
    if s.lis == nil {
        // Serve called after Stop or GracefulStop.
        s.mu.Unlock()
        lis.Close()
        return ErrServerStopped
    }

    s.serveWG.Add(1)
    defer func() {
        s.serveWG.Done()
        if s.quit.HasFired() {
            // Stop or GracefulStop called; block until done and return nil.
            <-s.done.Done()
        }
    }()

    ls := &listenSocket{Listener: lis}
    s.lis[ls] = true

    if channelz.IsOn() {
        ls.channelzID = channelz.RegisterListenSocket(ls, s.channelzID, lis.Addr().String())
    }
    s.mu.Unlock()

    defer func() {
        s.mu.Lock()
        if s.lis != nil && s.lis[ls] {
            ls.Close()
            delete(s.lis, ls)
        }
        s.mu.Unlock()
    }()

    var tempDelay time.Duration // how long to sleep on accept failure

    for {
        rawConn, err := lis.Accept()
        if err != nil {
            if ne, ok := err.(interface {
                Temporary() bool
            }); ok && ne.Temporary() {
                if tempDelay == 0 {
                    tempDelay = 5 * time.Millisecond
                } else {
                    tempDelay *= 2
                }
                if max := 1 * time.Second; tempDelay > max {
                    tempDelay = max
                }
                s.mu.Lock()
                s.printf("Accept error: %v; retrying in %v", err, tempDelay)
                s.mu.Unlock()
                timer := time.NewTimer(tempDelay)
                select {
                case <-timer.C:
                case <-s.quit.Done():
                    timer.Stop()
                    return nil
                }
                continue
            }
            s.mu.Lock()
            s.printf("done serving; Accept = %v", err)
            s.mu.Unlock()

            if s.quit.HasFired() {
                return nil
            }
            return err
        }
        tempDelay = 0
        // Start a new goroutine to deal with rawConn so we don't stall this Accept
        // loop goroutine.
        //
        // Make sure we account for the goroutine so GracefulStop doesn't nil out
        // s.conns before this conn can be added.
        s.serveWG.Add(1)
        go func() {
            s.handleRawConn(rawConn)
            s.serveWG.Done()
        }()
    }
}
```

### Prepare for stream

- `handleRawConn()` calls `s.useTransportAuthenticator()` first.
  - `s.useTransportAuthenticator()` is actually `Server.useTransportAuthenticator()`
  - `Server.useTransportAuthenticator()` performs TLS handshaking.
- `handleRawConn()` calls `s.newHTTP2Transport()`, which is actually `Server.newHTTP2Transport()`.
  - `Server.newHTTP2Transport()` calls `transport.NewServerTransport()`, which is actually `NewServerTransport()`.
  - `NewServerTransport()` calls `newHTTP2Server()`, which constructs a `ServerTransport` based on HTTP2.
  - `newHTTP2Server()` sends the server side SETTING frame to the client.
  - `newHTTP2Server()` adjusts the connection flow control window if needed.
  - `newHTTP2Server()` creates the `controlBuffer`.
  - For now, just remember `controlBuffer` is a buffer to communicate with client. See [controlBuffer and loopy](control.md) for detail.
  - `newHTTP2Server()` checks and validates the (HTTP 2) connection preface magic string.
  - `newHTTP2Server()` receives and processes client SETTING frame
  - `newHTTP2Server()` creates the `http2Server`, which implements the `ServerTransport` interface with HTTP2.
  - `newHTTP2Server()` starts `loopyWriter` in a new goroutine.
  - For now, just remember `loopyWriter` works closely with `controlBuffer` to process HTTP 2 frames.
  - `newHTTP2Server()` launches a new `t.keepalive()` goroutine to manage the connection state (e.g. close idle connection, keep alive active connection, etc.)
- `handleRawConn()` starts a new goroutine `s.serveStreams()` to process the stream.
  - Please note  `s.serveStreams()` runs in a separate goroutine.
  - `s.serveStreams()` is actually `Server.serveStreams()`.

For readers who are familiar with HTTP 2 protocol will find the above work is essential before accepting the client request. In short , the `s.newHTTP2Transport()` prepares for processing stream.

Next, let's discuss `Server.serveStreams()` in detail.

```go
// handleRawConn forks a goroutine to handle a just-accepted connection that
// has not had any I/O performed on it yet.
func (s *Server) handleRawConn(rawConn net.Conn) {
    if s.quit.HasFired() {
        rawConn.Close()
        return
    }
    rawConn.SetDeadline(time.Now().Add(s.opts.connectionTimeout))
    conn, authInfo, err := s.useTransportAuthenticator(rawConn)
    if err != nil {
        // ErrConnDispatched means that the connection was dispatched away from
        // gRPC; those connections should be left open.
        if err != credentials.ErrConnDispatched {
            s.mu.Lock()
            s.errorf("ServerHandshake(%q) failed: %v", rawConn.RemoteAddr(), err)
            s.mu.Unlock()
            channelz.Warningf(logger, s.channelzID, "grpc: Server.Serve failed to complete security handshake from %q: %v", rawConn.RemoteAddr(), err)
            rawConn.Close()
        }
        rawConn.SetDeadline(time.Time{})
        return
    }
 
    // Finish handshaking (HTTP2)
    st := s.newHTTP2Transport(conn, authInfo)
    if st == nil {
        return
    }
 
    rawConn.SetDeadline(time.Time{})
    if !s.addConn(st) {
        return
    }
    go func() {
        s.serveStreams(st)
        s.removeConn(st)
    }()
}

// newHTTP2Transport sets up a http/2 transport (using the
// gRPC http2 server transport in transport/http2_server.go).
func (s *Server) newHTTP2Transport(c net.Conn, authInfo credentials.AuthInfo) transport.ServerTransport {
    config := &transport.ServerConfig{
        MaxStreams:            s.opts.maxConcurrentStreams,
        AuthInfo:              authInfo,
        InTapHandle:           s.opts.inTapHandle,
        StatsHandler:          s.opts.statsHandler,
        KeepaliveParams:       s.opts.keepaliveParams,
        KeepalivePolicy:       s.opts.keepalivePolicy,
        InitialWindowSize:     s.opts.initialWindowSize,
        InitialConnWindowSize: s.opts.initialConnWindowSize,
        WriteBufferSize:       s.opts.writeBufferSize,
        ReadBufferSize:        s.opts.readBufferSize,
        ChannelzParentID:      s.channelzID,
        MaxHeaderListSize:     s.opts.maxHeaderListSize,
        HeaderTableSize:       s.opts.headerTableSize,
    }
    st, err := transport.NewServerTransport("http2", c, config)
    if err != nil {
        s.mu.Lock()
        s.errorf("NewServerTransport(%q) failed: %v", c.RemoteAddr(), err)
        s.mu.Unlock()
        c.Close()
        channelz.Warning(logger, s.channelzID, "grpc: Server.Serve failed to create ServerTransport: ", err)
        return nil
    }

    return st
}

// NewServerTransport creates a ServerTransport with conn or non-nil error
// if it fails.
func NewServerTransport(protocol string, conn net.Conn, config *ServerConfig) (ServerTransport, error) {
    return newHTTP2Server(conn, config)
}

// newHTTP2Server constructs a ServerTransport based on HTTP2. ConnectionError is
// returned if something goes wrong.
func newHTTP2Server(conn net.Conn, config *ServerConfig) (_ ServerTransport, err error) {
    writeBufSize := config.WriteBufferSize
    readBufSize := config.ReadBufferSize
    maxHeaderListSize := defaultServerMaxHeaderListSize
    if config.MaxHeaderListSize != nil {
        maxHeaderListSize = *config.MaxHeaderListSize
    }
    framer := newFramer(conn, writeBufSize, readBufSize, maxHeaderListSize)
    // Send initial settings as connection preface to client.
    isettings := []http2.Setting{{
        ID:  http2.SettingMaxFrameSize,
        Val: http2MaxFrameLen,
    }}
    // TODO(zhaoq): Have a better way to signal "no limit" because 0 is
    // permitted in the HTTP2 spec.
    maxStreams := config.MaxStreams
    if maxStreams == 0 {
        maxStreams = math.MaxUint32
    } else {
        isettings = append(isettings, http2.Setting{
            ID:  http2.SettingMaxConcurrentStreams,
            Val: maxStreams,
        })
    }
    dynamicWindow := true
    iwz := int32(initialWindowSize)
    if config.InitialWindowSize >= defaultWindowSize {
        iwz = config.InitialWindowSize
        dynamicWindow = false
    }
    icwz := int32(initialWindowSize)
    if config.InitialConnWindowSize >= defaultWindowSize {
        icwz = config.InitialConnWindowSize
        dynamicWindow = false
    }
    if iwz != defaultWindowSize {
        isettings = append(isettings, http2.Setting{
            ID:  http2.SettingInitialWindowSize,
            Val: uint32(iwz)})
    }
    if config.MaxHeaderListSize != nil {
        isettings = append(isettings, http2.Setting{
            ID:  http2.SettingMaxHeaderListSize,
            Val: *config.MaxHeaderListSize,
        })
    }
    if config.HeaderTableSize != nil {
        isettings = append(isettings, http2.Setting{
            ID:  http2.SettingHeaderTableSize,
            Val: *config.HeaderTableSize,
        })
    }
    if err := framer.fr.WriteSettings(isettings...); err != nil {
        return nil, connectionErrorf(false, err, "transport: %v", err)
    }
    // Adjust the connection flow control window if needed.
    if delta := uint32(icwz - defaultWindowSize); delta > 0 {
        if err := framer.fr.WriteWindowUpdate(0, delta); err != nil {
            return nil, connectionErrorf(false, err, "transport: %v", err)
        }
    }
    kp := config.KeepaliveParams
    if kp.MaxConnectionIdle == 0 {
        kp.MaxConnectionIdle = defaultMaxConnectionIdle
    }
    if kp.MaxConnectionAge == 0 {
        kp.MaxConnectionAge = defaultMaxConnectionAge
    }
    // Add a jitter to MaxConnectionAge.
    kp.MaxConnectionAge += getJitter(kp.MaxConnectionAge)
    if kp.MaxConnectionAgeGrace == 0 {
        kp.MaxConnectionAgeGrace = defaultMaxConnectionAgeGrace
    }
    if kp.Time == 0 {
        kp.Time = defaultServerKeepaliveTime
    }
    if kp.Timeout == 0 {
        kp.Timeout = defaultServerKeepaliveTimeout
    }
    kep := config.KeepalivePolicy
    if kep.MinTime == 0 {
        kep.MinTime = defaultKeepalivePolicyMinTime
    }
    done := make(chan struct{})
    t := &http2Server{
        ctx:               context.Background(),
        done:              done,
        conn:              conn,
        remoteAddr:        conn.RemoteAddr(),
        localAddr:         conn.LocalAddr(),
        authInfo:          config.AuthInfo,
        framer:            framer,
        readerDone:        make(chan struct{}),
        writerDone:        make(chan struct{}),
        maxStreams:        maxStreams,
        inTapHandle:       config.InTapHandle,
        fc:                &trInFlow{limit: uint32(icwz)},
        state:             reachable,
        activeStreams:     make(map[uint32]*Stream),
        stats:             config.StatsHandler,
        kp:                kp,
        idle:              time.Now(),
        kep:               kep,
        initialWindowSize: iwz,
        czData:            new(channelzData),
        bufferPool:        newBufferPool(),
    }
    t.controlBuf = newControlBuffer(t.done)
    if dynamicWindow {
        t.bdpEst = &bdpEstimator{
            bdp:               initialWindowSize,
            updateFlowControl: t.updateFlowControl,
        }
    }
    if t.stats != nil {
        t.ctx = t.stats.TagConn(t.ctx, &stats.ConnTagInfo{
            RemoteAddr: t.remoteAddr,
            LocalAddr:  t.localAddr,
        })
        connBegin := &stats.ConnBegin{}
        t.stats.HandleConn(t.ctx, connBegin)
    }
    if channelz.IsOn() {
        t.channelzID = channelz.RegisterNormalSocket(t, config.ChannelzParentID, fmt.Sprintf("%s -> %s", t.remoteAddr, t.localAddr))
    }

    t.connectionID = atomic.AddUint64(&serverConnectionCounter, 1)

    t.framer.writer.Flush()

    defer func() {
        if err != nil {
            t.Close()
        }
    }()

    // Check the validity of client preface.
    preface := make([]byte, len(clientPreface))
    if _, err := io.ReadFull(t.conn, preface); err != nil {
        return nil, connectionErrorf(false, err, "transport: http2Server.HandleStreams failed to receive the preface from client: %v", err)
    }
    if !bytes.Equal(preface, clientPreface) {
        return nil, connectionErrorf(false, nil, "transport: http2Server.HandleStreams received bogus greeting from client: %q", preface)
    }

    frame, err := t.framer.fr.ReadFrame()
    if err == io.EOF || err == io.ErrUnexpectedEOF {
        return nil, err
    }
    if err != nil {
        return nil, connectionErrorf(false, err, "transport: http2Server.HandleStreams failed to read initial settings frame: %v", err)
    }
    atomic.StoreInt64(&t.lastRead, time.Now().UnixNano())
    sf, ok := frame.(*http2.SettingsFrame)
    if !ok {
        return nil, connectionErrorf(false, nil, "transport: http2Server.HandleStreams saw invalid preface type %T from client", frame)
    }
    t.handleSettings(sf)

    go func() {
        t.loopy = newLoopyWriter(serverSide, t.framer, t.controlBuf, t.bdpEst)
        t.loopy.ssGoAwayHandler = t.outgoingGoAwayHandler
        if err := t.loopy.run(); err != nil {
            if logger.V(logLevel) {
                logger.Errorf("transport: loopyWriter.run returning. Err: %v", err)
            }
        }
        t.conn.Close()
        close(t.writerDone)
    }()
    go t.keepalive()
    return t, nil
}
```

### Serve stream

- `serveStreams()` call the `st.HandleStreams()` to finish the job.
- `st.HandleStreams()` is actually `http2Server.HandleStreams()`.
- `st.HandleStreams()` has two parameters, one is `handle` function, the other is `traceCtx` function.
- `serveStreams()` provides two anonymous functions as the arguments for `st.HandleStreams()`.
  - The `handle` function wrappers `s.handleStream()` and tries to send `serverWorkerData` to channel `s.serverWorkerChannels[]`.
    - Please note `s.handleStream()` runs in a separate goroutine.
    - `handle` checks the `s.opts.numServerWorkers` to determine if there is server workers configuration.
    - If the answer is yes, the wrapper function will try to send the `serverWorkerData` to one of the server worker through channel `s.serverWorkerChannels[?]`.
    - The wrapper use round robin policy to pick up the server worker.
    - If there is no server worker configuration or the chosen server worker is busy, the wrapper function start a new goroutine and call `s.handleStream()`.
    - Please see [Server worker](#server-worker) for detail.
  - The `traceCtx` function provides tracing information to the context.

Next, let's discuss `http2Server.HandleStreams()` in detail.

```go
func (s *Server) serveStreams(st transport.ServerTransport) {
    defer st.Close()
    var wg sync.WaitGroup

    var roundRobinCounter uint32
    st.HandleStreams(func(stream *transport.Stream) {
        wg.Add(1)
        if s.opts.numServerWorkers > 0 {
            data := &serverWorkerData{st: st, wg: &wg, stream: stream}
            select {
            case s.serverWorkerChannels[atomic.AddUint32(&roundRobinCounter, 1)%s.opts.numServerWorkers] <- data:
            default:
                // If all stream workers are busy, fallback to the default code path.
                go func() {
                    s.handleStream(st, stream, s.traceInfo(st, stream))
                    wg.Done()
                }()
            }
        } else {
            go func() {
                defer wg.Done()
                s.handleStream(st, stream, s.traceInfo(st, stream))
            }()
        }
    }, func(ctx context.Context, method string) context.Context {
        if !EnableTracing {
            return ctx
        }
        tr := trace.New("grpc.Recv."+methodFamily(method), method)
        return trace.NewContext(ctx, tr)
    })
    wg.Wait()
}

```

### Read frame

`HandleStreams()` keeps reading HTTP2 frames and process frames according to its type.

- `HandleStreams()` calls `t.framer.fr.ReadFrame()` to read HTTP2 frames.
  - `t.framer.fr.ReadFrame()` is actually `Framer.ReadFrame()`.
- `HandleStreams()` processes the read frames according to frame type.
  - We will focus on `http2.MetaHeadersFrame`, which is the key for next step.
  - For `MetaHeadersFrame` type, `HandleStreams()` calls `t.operateHeaders()` to process it. `t.operateHeaders()` is actually `http2Server.operateHeaders()`.
  - For other frame type, `HandleStreams()` will process it also. Including  `PingFrame`, `WindowUpdateFrame`, `GoAwayFrame`, `SettingsFrame`, even `RSTStreamFrame` in some case.
- `HandleStreams()` continues to read more frames until the stream is closed.

Please note if `ReadFrame()` return `MetaHeadersFrame` type, the `Request-Headers` part is received by the server.

Actually `Framer.ReadFrame()` can read all kinds of frame. It perform the following work:

- `Framer.ReadFrame()` reads frame header
- `Framer.ReadFrame()` reads frame payload data
- `Framer.ReadFrame()` parses the payload data to build different frame
- `Framer.ReadFrame()` checks frame order to find invalid frame, mainly checks whether HEADERS and CONTINUATION frames are contiguous.
- `Framer.ReadFrame()` reads 0 or more CONTINUATION frames and merges them into `MetaHeadersFrame`.

`MetaHeadersFrame` is a special Frame. It represent the start point of gRPC method call. There is no `MetaHeadersFrame` in HTTP 2 protocol. For gRPC calling, process one HEADER frame plus zero or more CONTINUATION frames and some DATA frames will not be easy. By this design, the for loop in `HandleStreams()`, `MetaHeadersFrame` contains all the method call information except method call parameter. The method call parameters exists in Data frames.

Right now, we don't need data frame immediately, gRPC can process the `MetaHeadersFrame` by calling `http2Server.operateHeaders()`. Next, let's discuss the `http2Server.operateHeaders()` in detail.

Eventually, we still need the method call parameters. The `DataFrame` processing is a little bit complex. Let's make it clear in advance:

- `HandleStreams()` runs in a separated goroutine.
  - `HandleStreams()` reads the data frame which contains the method call parameter.
  - For data frame type, `HandleStreams()` calls `t.handleData()` to send the data frame payload to `s.handleStream()` through a channel.
- `s.handleStream()` runs in a separated goroutine.
  - `s.handleStream()` will block on a channel to wait for the method call parameter.
  - `s.handleStream()` gets the method call parameter, continue the process.

How the two goroutine communicate with each other deserves another chapter. Please see [Request parameter](parameters.md) for detail.

```go
// HandleStreams receives incoming streams using the given handler. This is
// typically run in a separate goroutine.
// traceCtx attaches trace to ctx and returns the new context.
func (t *http2Server) HandleStreams(handle func(*Stream), traceCtx func(context.Context, string) context.Context) {
    defer close(t.readerDone)
    for {
        t.controlBuf.throttle()
        frame, err := t.framer.fr.ReadFrame()
        atomic.StoreInt64(&t.lastRead, time.Now().UnixNano())
        if err != nil {
            if se, ok := err.(http2.StreamError); ok {
                if logger.V(logLevel) {
                    logger.Warningf("transport: http2Server.HandleStreams encountered http2.StreamError: %v", se)
                }
                t.mu.Lock()
                s := t.activeStreams[se.StreamID]
                t.mu.Unlock()
                if s != nil {
                    t.closeStream(s, true, se.Code, false)
                } else {
                    t.controlBuf.put(&cleanupStream{
                        streamID: se.StreamID,
                        rst:      true,
                        rstCode:  se.Code,
                        onWrite:  func() {},
                    })
                }
                continue
            }
            if err == io.EOF || err == io.ErrUnexpectedEOF {
                t.Close()
                return
            }
            if logger.V(logLevel) {
                logger.Warningf("transport: http2Server.HandleStreams failed to read frame: %v", err)
            }
            t.Close()
            return
        }
        switch frame := frame.(type) {
        case *http2.MetaHeadersFrame:
            if t.operateHeaders(frame, handle, traceCtx) {
                t.Close()
                break
            }
        case *http2.DataFrame:
            t.handleData(frame)
        case *http2.RSTStreamFrame:
            t.handleRSTStream(frame)
        case *http2.SettingsFrame:
            t.handleSettings(frame)
        case *http2.PingFrame:
            t.handlePing(frame)
        case *http2.WindowUpdateFrame:
            t.handleWindowUpdate(frame)
        case *http2.GoAwayFrame:
            // TODO: Handle GoAway from the client appropriately.
        default:
            if logger.V(logLevel) {
                logger.Errorf("transport: http2Server.HandleStreams found unhandled frame type %v.", frame)
            }
        }
    }
}

// ReadFrame reads a single frame. The returned Frame is only valid
// until the next call to ReadFrame.
//
// If the frame is larger than previously set with SetMaxReadFrameSize, the
// returned error is ErrFrameTooLarge. Other errors may be of type
// ConnectionError, StreamError, or anything else from the underlying
// reader.
func (fr *Framer) ReadFrame() (Frame, error) {
    fr.errDetail = nil
    if fr.lastFrame != nil {
        fr.lastFrame.invalidate()
    }
    fh, err := readFrameHeader(fr.headerBuf[:], fr.r)
    if err != nil {
        return nil, err
    }
    if fh.Length > fr.maxReadSize {
        return nil, ErrFrameTooLarge
    }
    payload := fr.getReadBuf(fh.Length)
    if _, err := io.ReadFull(fr.r, payload); err != nil {
        return nil, err
    }
    f, err := typeFrameParser(fh.Type)(fr.frameCache, fh, payload)
    if err != nil {
        if ce, ok := err.(connError); ok {
            return nil, fr.connError(ce.Code, ce.Reason)
        }
        return nil, err
    }
    if err := fr.checkFrameOrder(f); err != nil {
        return nil, err
    }
    if fr.logReads {
        fr.debugReadLoggerf("http2: Framer %p: read %v", fr, summarizeFrame(f))
    }
    if fh.Type == FrameHeaders && fr.ReadMetaHeaders != nil {
        return fr.readMetaFrame(f.(*HeadersFrame))
    }
    return f, nil
}
```

### Decode header

`operateHeaders()` creates stream based on information provided by `MetaHeadersFrame`. Then it checks the stream to make sure it's validity. The final step is calling `handle()` to process the gRPC request.

- `http2Server.operateHeaders()` calls `state.decodeHeader()` to decode the HTTP headers from `MetaHeadersFrame`.
  - `state.decodeHeader()` is actually `decodeState.decodeHeader()`.
  - For each header field in frame, `decodeHeader()` calls `d.processHeaderField()` to decode it. `d.processHeaderField()` is `decodeState.processHeaderField()`.
  - In `processHeaderField()`, please note the `":path"` field, which determines the service name and method name.
  - Please refer to [gRPC over HTTP2](https://github.com/grpc/grpc/blob/master/doc/PROTOCOL-HTTP2.md) for the field explanation.
- `http2Server.operateHeaders()` creates `Stream` based on the HTTP headers.
- `http2Server.operateHeaders()` calls `t.controlBuf.put()` to register the stream with loopy.
- `http2Server.operateHeaders()` calls `handle()`, which is actually the `handle` function in `Server.serveStreams()`. See [Serve stream](#serve-stream) for detail.
  - `handle` function wrappers `s.handleStream()`.
  - `s.handleStream()` is actually `Server.handleStream()`.

Next, we will discuss server worker mechanism first, then we will continue the discussion of `Server.handleStream()`.

```go
// operateHeader takes action on the decoded headers.
func (t *http2Server) operateHeaders(frame *http2.MetaHeadersFrame, handle func(*Stream), traceCtx func(context.Context, string) context.Context) (fatal bool) {
    streamID := frame.Header().StreamID
    state := &decodeState{
        serverSide: true,
    }
    if h2code, err := state.decodeHeader(frame); err != nil {
        if _, ok := status.FromError(err); ok {
            t.controlBuf.put(&cleanupStream{
                streamID: streamID,
                rst:      true,
                rstCode:  h2code,
                onWrite:  func() {},
            })
        }
        return false
    }

    buf := newRecvBuffer()
    s := &Stream{
        id:             streamID,
        st:             t,
        buf:            buf,
        fc:             &inFlow{limit: uint32(t.initialWindowSize)},
        recvCompress:   state.data.encoding,
        method:         state.data.method,
        contentSubtype: state.data.contentSubtype,
    }
    if frame.StreamEnded() {
        // s is just created by the caller. No lock needed.
        s.state = streamReadDone
    }
    if state.data.timeoutSet {
        s.ctx, s.cancel = context.WithTimeout(t.ctx, state.data.timeout)
    } else {
        s.ctx, s.cancel = context.WithCancel(t.ctx)
    }
    pr := &peer.Peer{
        Addr: t.remoteAddr,
    }
    // Attach Auth info if there is any.
    if t.authInfo != nil {
        pr.AuthInfo = t.authInfo
    }
    s.ctx = peer.NewContext(s.ctx, pr)
    // Attach the received metadata to the context.
    if len(state.data.mdata) > 0 {
        s.ctx = metadata.NewIncomingContext(s.ctx, state.data.mdata)
    }
    if state.data.statsTags != nil {
        s.ctx = stats.SetIncomingTags(s.ctx, state.data.statsTags)
    }
    if state.data.statsTrace != nil {
        s.ctx = stats.SetIncomingTrace(s.ctx, state.data.statsTrace)
    }
    if t.inTapHandle != nil {
        var err error
        info := &tap.Info{
            FullMethodName: state.data.method,
        }
        s.ctx, err = t.inTapHandle(s.ctx, info)
        if err != nil {
            if logger.V(logLevel) {
                logger.Warningf("transport: http2Server.operateHeaders got an error from InTapHandle: %v", err)
            }
            t.controlBuf.put(&cleanupStream{
                streamID: s.id,
                rst:      true,
                rstCode:  http2.ErrCodeRefusedStream,
                onWrite:  func() {},
            })
            s.cancel()
            return false
        }
    }
    t.mu.Lock()
    if t.state != reachable {
        t.mu.Unlock()
        s.cancel()
        return false
    }
    if uint32(len(t.activeStreams)) >= t.maxStreams {
        t.mu.Unlock()
        t.controlBuf.put(&cleanupStream{
            streamID: streamID,
            rst:      true,
            rstCode:  http2.ErrCodeRefusedStream,
            onWrite:  func() {},
        })
        s.cancel()
        return false
    }
    if streamID%2 != 1 || streamID <= t.maxStreamID {
        t.mu.Unlock()
        // illegal gRPC stream id.
        if logger.V(logLevel) {
            logger.Errorf("transport: http2Server.HandleStreams received an illegal stream id: %v", streamID)
        }
        s.cancel()
        return true
    }
    t.maxStreamID = streamID
    t.activeStreams[streamID] = s
    if len(t.activeStreams) == 1 {
        t.idle = time.Time{}
    }
    t.mu.Unlock()
    if channelz.IsOn() {
        atomic.AddInt64(&t.czData.streamsStarted, 1)
        atomic.StoreInt64(&t.czData.lastStreamCreatedTime, time.Now().UnixNano())
    }
    s.requestRead = func(n int) {
        t.adjustWindow(s, uint32(n))
    }
    s.ctx = traceCtx(s.ctx, s.method)
    if t.stats != nil {
        s.ctx = t.stats.TagRPC(s.ctx, &stats.RPCTagInfo{FullMethodName: s.method})
        inHeader := &stats.InHeader{
            FullMethod:  s.method,
            RemoteAddr:  t.remoteAddr,
            LocalAddr:   t.localAddr,
            Compression: s.recvCompress,
            WireLength:  int(frame.Header().Length),
            Header:      metadata.MD(state.data.mdata).Copy(),
        }
        t.stats.HandleRPC(s.ctx, inHeader)
    }
    s.ctxDone = s.ctx.Done()
    s.wq = newWriteQuota(defaultWriteQuota, s.ctxDone)
    s.trReader = &transportReader{
        reader: &recvBufferReader{
            ctx:        s.ctx,
            ctxDone:    s.ctxDone,
            recv:       s.buf,
            freeBuffer: t.bufferPool.put,
        },
        windowHandler: func(n int) {
            t.updateWindow(s, uint32(n))
        },
    }
    // Register the stream with loopy.
    t.controlBuf.put(&registerStream{
        streamID: s.id,
        wq:       s.wq,
    })
    handle(s)
    return false
}

func (d *decodeState) ddecodeHeaderecodeHeader(frame *http2.MetaHeadersFrame) (http2.ErrCode, error) {
    // frame.Truncated is set to true when framer detects that the current header
    // list size hits MaxHeaderListSize limit.
    if frame.Truncated {
        return http2.ErrCodeFrameSize, status.Error(codes.Internal, "peer header list size exceeded limit")
    }

    for _, hf := range frame.Fields {
        d.processHeaderField(hf)
    }

    if d.data.isGRPC {
        if d.data.grpcErr != nil {
            return http2.ErrCodeProtocol, d.data.grpcErr
        }
        if d.serverSide {
            return http2.ErrCodeNo, nil
        }
        if d.data.rawStatusCode == nil && d.data.statusGen == nil {
            // gRPC status doesn't exist.
            // Set rawStatusCode to be unknown and return nil error.
            // So that, if the stream has ended this Unknown status
            // will be propagated to the user.
            // Otherwise, it will be ignored. In which case, status from
            // a later trailer, that has StreamEnded flag set, is propagated.
            code := int(codes.Unknown)
            d.data.rawStatusCode = &code
        }
        return http2.ErrCodeNo, nil
    }

    // HTTP fallback mode
    if d.data.httpErr != nil {
        return http2.ErrCodeProtocol, d.data.httpErr
    }

    var (
        code = codes.Internal // when header does not include HTTP status, return INTERNAL
        ok   bool
    )

    if d.data.httpStatus != nil {
        code, ok = HTTPStatusConvTab[*(d.data.httpStatus)]
        if !ok {
            code = codes.Unknown
        }
    }

    return http2.ErrCodeProtocol, status.Error(code, d.constructHTTPErrMsg())
}

func (d *decodeState) processHeaderField(f hpack.HeaderField) {
    switch f.Name {
    case "content-type":
        contentSubtype, validContentType := grpcutil.ContentSubtype(f.Value)
        if !validContentType {
            d.data.contentTypeErr = fmt.Sprintf("transport: received the unexpected content-type %q", f.Value)
            return
        }
        d.data.contentSubtype = contentSubtype
        // TODO: do we want to propagate the whole content-type in the metadata,
        // or come up with a way to just propagate the content-subtype if it was set?
        // ie {"content-type": "application/grpc+proto"} or {"content-subtype": "proto"}
        // in the metadata?
        d.addMetadata(f.Name, f.Value)
        d.data.isGRPC = true
    case "grpc-encoding":
        d.data.encoding = f.Value
    case "grpc-status":
        code, err := strconv.Atoi(f.Value)
        if err != nil {
            d.data.grpcErr = status.Errorf(codes.Internal, "transport: malformed grpc-status: %v", err)
            return
        }
        d.data.rawStatusCode = &code
    case "grpc-message":
        d.data.rawStatusMsg = decodeGrpcMessage(f.Value)
    case "grpc-status-details-bin":
        v, err := decodeBinHeader(f.Value)
        if err != nil {
            d.data.grpcErr = status.Errorf(codes.Internal, "transport: malformed grpc-status-details-bin: %v", err)
            return
        }
        s := &spb.Status{}
        if err := proto.Unmarshal(v, s); err != nil {
            d.data.grpcErr = status.Errorf(codes.Internal, "transport: malformed grpc-status-details-bin: %v", err)
            return
        }
        d.data.statusGen = status.FromProto(s)
    case "grpc-timeout":
        d.data.timeoutSet = true
        var err error
        if d.data.timeout, err = decodeTimeout(f.Value); err != nil {
            d.data.grpcErr = status.Errorf(codes.Internal, "transport: malformed time-out: %v", err)
        }
    case ":path":
        d.data.method = f.Value
    case ":status":
        code, err := strconv.Atoi(f.Value)
        if err != nil {
            d.data.httpErr = status.Errorf(codes.Internal, "transport: malformed http-status: %v", err)
            return
        }
        d.data.httpStatus = &code
    case "grpc-tags-bin":
        v, err := decodeBinHeader(f.Value)
        if err != nil {
            d.data.grpcErr = status.Errorf(codes.Internal, "transport: malformed grpc-tags-bin: %v", err)
            return
        }
        d.data.statsTags = v
        d.addMetadata(f.Name, string(v))
    case "grpc-trace-bin":
        v, err := decodeBinHeader(f.Value)
        if err != nil {
            d.data.grpcErr = status.Errorf(codes.Internal, "transport: malformed grpc-trace-bin: %v", err)
            return
        }
        d.data.statsTrace = v
        d.addMetadata(f.Name, string(v))
    default:
        if isReservedHeader(f.Name) && !isWhitelistedHeader(f.Name) {
            break
        }
        v, err := decodeMetadataHeader(f.Name, f.Value)
        if err != nil {
            if logger.V(logLevel) {
                logger.Errorf("Failed to decode metadata header (%q, %q): %v", f.Name, f.Value, err)
            }
            return
        }
        d.addMetadata(f.Name, v)
    }
}
```

### Server worker

Before the discussion of `s.handleStream()`, let's spend some time to understand the server worker mechanism. It's time to see what happened when the `serverWorkerData` is sent to `s.serverWorkerChannels[]`.

- `Server.initServerWorkers()` creates worker goroutines and channels to process incoming connections to reduce the time spent overall.
  - `Server.initServerWorkers()` is controlled by `s.opts.numServerWorkers` in `NewServer()` function.
- `Server.serverWorker()` blocks on a `ch serverWorkerData` channel forever and waits for data to be fed by `serveStreams()`.
- `Server.serverWorker()` eventually calls `s.handleStream()` to process the stream.

`s.opts.numServerWorkers` can be set by `NumStreamWorkers()`. If `s.opts.numServerWorkers` is set, `s.initServerWorkers()` will be called during `NewServer` process. Inside `s.initServerWorkers()`, several `s.serverWorker()` goroutine will be started. `s.serverWorkerChannels[i]` is the communication channel.

`s.serverWorker()` goroutine blocks on the `ch chan *serverWorkerData` channel and wait for the data to be fed by `serveStreams()`. When the for loop runs `threshold` times, it will reset the `s.serverWorker()` goroutine. The purpose is to reset its stack so that large stacks don't live in memory forever.

Either invoke `s.handleStream()` directly or invoke `s.handleStream()` indirectly through `s.serverWorker()`, `s.handleStream()` is the key to handle the gRPC request. Remember that `s.handleStream()` always runs in its goroutine, whatever direct or indirect invocation.

```go
// NewServer creates a gRPC server which has no service registered and has not
// started to accept requests yet.
func NewServer(opt ...ServerOption) *Server {
    opts := defaultServerOptions
    for _, o := range opt {
        o.apply(&opts)
    }
    s := &Server{
        lis:      make(map[net.Listener]bool),
        opts:     opts,
        conns:    make(map[transport.ServerTransport]bool),
        services: make(map[string]*serviceInfo),
        quit:     grpcsync.NewEvent(),
        done:     grpcsync.NewEvent(),
        czData:   new(channelzData),
    }
    chainUnaryServerInterceptors(s)
    chainStreamServerInterceptors(s)
    s.cv = sync.NewCond(&s.mu)
    if EnableTracing {
        _, file, line, _ := runtime.Caller(1)
        s.events = trace.NewEventLog("grpc.Server", fmt.Sprintf("%s:%d", file, line))
    }

    if s.opts.numServerWorkers > 0 {
        s.initServerWorkers()
    }

    if channelz.IsOn() {
        s.channelzID = channelz.RegisterServer(&channelzServer{s}, "")
    }
    return s
}

// NumStreamWorkers returns a ServerOption that sets the number of worker
// goroutines that should be used to process incoming streams. Setting this to
// zero (default) will disable workers and spawn a new goroutine for each
// stream.
//
// Experimental
//
// Notice: This API is EXPERIMENTAL and may be changed or removed in a
// later release.
func NumStreamWorkers(numServerWorkers uint32) ServerOption {
    // TODO: If/when this API gets stabilized (i.e. stream workers become the
    // only way streams are processed), change the behavior of the zero value to
    // a sane default. Preliminary experiments suggest that a value equal to the
    // number of CPUs available is most performant; requires thorough testing.
    return newFuncServerOption(func(o *serverOptions) {
        o.numServerWorkers = numServerWorkers
    })
}

// serverWorkerResetThreshold defines how often the stack must be reset. Every
// N requests, by spawning a new goroutine in its place, a worker can reset its
// stack so that large stacks don't live in memory forever. 2^16 should allow
// each goroutine stack to live for at least a few seconds in a typical
// workload (assuming a QPS of a few thousand requests/sec).
const serverWorkerResetThreshold = 1 << 16

// serverWorkers blocks on a *transport.Stream channel forever and waits for
// data to be fed by serveStreams. This allows different requests to be
// processed by the same goroutine, removing the need for expensive stack
// re-allocations (see the runtime.morestack problem [1]).
//
// [1] https://github.com/golang/go/issues/18138
func (s *Server) serverWorker(ch chan *serverWorkerData) {
    // To make sure all server workers don't reset at the same time, choose a
    // random number of iterations before resetting.
    threshold := serverWorkerResetThreshold + grpcrand.Intn(serverWorkerResetThreshold)
    for completed := 0; completed < threshold; completed++ {
        data, ok := <-ch
        if !ok {
            return
        }
        s.handleStream(data.st, data.stream, s.traceInfo(data.st, data.stream))
        data.wg.Done()
    }
    go s.serverWorker(ch)
}

// initServerWorkers creates worker goroutines and channels to process incoming
// connections to reduce the time spent overall on runtime.morestack.
func (s *Server) initServerWorkers() {
    s.serverWorkerChannels = make([]chan *serverWorkerData, s.opts.numServerWorkers)
    for i := uint32(0); i < s.opts.numServerWorkers; i++ {
        s.serverWorkerChannels[i] = make(chan *serverWorkerData)
        go s.serverWorker(s.serverWorkerChannels[i])
    }
}

// Server is a gRPC server to serve RPC requests.
type Server struct {
    opts serverOptions

    mu       sync.Mutex // guards following
    lis      map[net.Listener]bool
    conns    map[transport.ServerTransport]bool
    serve    bool
    drain    bool
    cv       *sync.Cond              // signaled when connections close for GracefulStop
    services map[string]*serviceInfo // service name -> service info
    events   trace.EventLog

    quit               *grpcsync.Event
    done               *grpcsync.Event
    channelzRemoveOnce sync.Once
    serveWG            sync.WaitGroup // counts active Serve goroutines for GracefulStop

    channelzID int64 // channelz unique identification number
    czData     *channelzData

    serverWorkerChannels []chan *serverWorkerData
}

type serverWorkerData struct {
    st     transport.ServerTransport
    wg     *sync.WaitGroup
    stream *transport.Stream
}
```

### Handle request

When `operateHeaders()` creates stream for the request, one important step is to decode request header from header frame.

Here is the request header example:

```txt
:method = POST 
:scheme = http 
:path = /helloworld.Greeter/SayHello 
:authority = localhost
te = trailers 
grpc-timeout = 1S 
content-type = application/grpc+proto
grpc-encoding = gzip 
authorization = Bearer xxxxxx 
```

At the last step, `operateHeaders()` calls `handle()`, which indirectly calls `Server.handleStream()`. Here the value of the `:path` field is assigned to `stream.method`, now `stream.method` contains the service name and method name:

```go
stream.method = "/helloworld.Greeter/SayHello"
```

Please refer to [Send header](request.md#send-header) to know how to set the `":path"` header filed.

- `handleStream()` splits `stream.Method()` into `servce` and `method`.
- Then `handleStream()` tries to find the registered `methodHandler` with `service="helloworld.Greeter"` and `method="SayHello"` in `s.services`.
- If success, `handleStream()` will find the `_Greeter_SayHello_Handler`. Please refer to [Register service](#register-service) for detail.  
  - for unary RPC, `handleStream()` calls `s.processUnaryRPC()`.
  - for stream RPC, `handleStream()` calls `s.processStreamingRPC()`.
- If not, `handleStream()` provides a solution for unknown service and/or unknown method.

Next, let's discuss `s.processUnaryRPC()` in detail.

```go
func (s *Server) handleStream(t transport.ServerTransport, stream *transport.Stream, trInfo *traceInfo) {
    sm := stream.Method()
    if sm != "" && sm[0] == '/' {
        sm = sm[1:]
    }
    pos := strings.LastIndex(sm, "/")
    if pos == -1 {
        if trInfo != nil {
            trInfo.tr.LazyLog(&fmtStringer{"Malformed method name %q", []interface{}{sm}}, true)
            trInfo.tr.SetError()
        }
        errDesc := fmt.Sprintf("malformed method name: %q", stream.Method())
        if err := t.WriteStatus(stream, status.New(codes.ResourceExhausted, errDesc)); err != nil {
            if trInfo != nil {
                trInfo.tr.LazyLog(&fmtStringer{"%v", []interface{}{err}}, true)
                trInfo.tr.SetError()
            }
            channelz.Warningf(logger, s.channelzID, "grpc: Server.handleStream failed to write status: %v", err)
        }
        if trInfo != nil {
            trInfo.tr.Finish()
        }
        return
    }
    service := sm[:pos]
    method := sm[pos+1:]

    srv, knownService := s.services[service]
    if knownService {
        if md, ok := srv.methods[method]; ok {
            s.processUnaryRPC(t, stream, srv, md, trInfo)
            return
        }
        if sd, ok := srv.streams[method]; ok {
            s.processStreamingRPC(t, stream, srv, sd, trInfo)
            return
        }
    }
    // Unknown service, or known server unknown method.
    if unknownDesc := s.opts.unknownStreamDesc; unknownDesc != nil {
        s.processStreamingRPC(t, stream, nil, unknownDesc, trInfo)
        return
    }
    var errDesc string
    if !knownService {
        errDesc = fmt.Sprintf("unknown service %v", service)
    } else {
        errDesc = fmt.Sprintf("unknown method %v for service %v", method, service)
    }
    if trInfo != nil {
        trInfo.tr.LazyPrintf("%s", errDesc)
        trInfo.tr.SetError()
    }
    if err := t.WriteStatus(stream, status.New(codes.Unimplemented, errDesc)); err != nil {
        if trInfo != nil {
            trInfo.tr.LazyLog(&fmtStringer{"%v", []interface{}{err}}, true)
            trInfo.tr.SetError()
        }processUnaryRPC
        channelz.Warningf(logger, s.channelzID, "grpc: Server.handleStream failed to write status: %v", err)
    }
    if trInfo != nil {
        trInfo.tr.Finish()
    }
}
```

### Process unary RPC

`Server.processUnaryRPC()` indirectly reads the request data frame, executes the request method calling and sends the response back to the client.

- `processUnaryRPC()` prepares the compression and decompression object.
- `processUnaryRPC()` calls `recvAndDecompress()` to get the RPC method parameters: `d`.
  - Here the `Length-Prefixed-Message` will be read.
- `processUnaryRPC()` prepares the decode function `df` for method handler.
- `processUnaryRPC()` calls `md.Handler()` to process the RPC method calling, the response is `reply`.
- `processUnaryRPC()` calls `s.sendResponse()` to send the response to client.
  - Here the `Response-Headers` and `Length-Prefixed-Message` will be sent.
- `processUnaryRPC()` call `t.WriteStatus()` to send the trailer to end the stream.
  - Here the `Trailers` will be sent.

Now we know the whole picture, let's dive each part one by one. First let's discuss `recvAndDecompress()`.

```go
func (s *Server) processUnaryRPC(t transport.ServerTransport, stream *transport.Stream, info *serviceInfo, md *MethodDesc, trInfo *traceInfo) (err error) {
    sh := s.opts.statsHandler
    if sh != nil || trInfo != nil || channelz.IsOn() {
        if channelz.IsOn() {
            s.incrCallsStarted()
        }
        var statsBegin *stats.Begin
        if sh != nil {
            beginTime := time.Now()
            statsBegin = &stats.Begin{
                BeginTime: beginTime,
            }
            sh.HandleRPC(stream.Context(), statsBegin)
        }
        if trInfo != nil {
            trInfo.tr.LazyLog(&trInfo.firstLine, false)
        }
        // The deferred error handling for tracing, stats handler and channelz are
        // combined into one function to reduce stack usage -- a defer takes ~56-64
        // bytes on the stack, so overflowing the stack will require a stack
        // re-allocation, which is expensive.
        //
        // To maintain behavior similar to separate deferred statements, statements
        // should be executed in the reverse order. That is, tracing first, stats
        // handler second, and channelz last. Note that panics *within* defers will
        // lead to different behavior, but that's an acceptable compromise; that
        // would be undefined behavior territory anyway.
        defer func() {
            if trInfo != nil {
                if err != nil && err != io.EOF {
                    trInfo.tr.LazyLog(&fmtStringer{"%v", []interface{}{err}}, true)
                    trInfo.tr.SetError()
                }
                trInfo.tr.Finish()
            }

            if sh != nil {
                end := &stats.End{
                    BeginTime: statsBegin.BeginTime,
                    EndTime:   time.Now(),
                }
                if err != nil && err != io.EOF {
                    end.Error = toRPCErr(err)
                }
                sh.HandleRPC(stream.Context(), end)
            }

            if channelz.IsOn() {
                if err != nil && err != io.EOF {
                    s.incrCallsFailed()
                } else {
                    s.incrCallsSucceeded()
                }
            }
        }()
    }

    binlog := binarylog.GetMethodLogger(stream.Method())
    if binlog != nil {
        ctx := stream.Context()
        md, _ := metadata.FromIncomingContext(ctx)
        logEntry := &binarylog.ClientHeader{
            Header:     md,
            MethodName: stream.Method(),
            PeerAddr:   nil,
        }
        if deadline, ok := ctx.Deadline(); ok {
            logEntry.Timeout = time.Until(deadline)
            if logEntry.Timeout < 0 {
                logEntry.Timeout = 0
            }
        }
        if a := md[":authority"]; len(a) > 0 {
            logEntry.Authority = a[0]
        }
        if peer, ok := peer.FromContext(ctx); ok {
            logEntry.PeerAddr = peer.Addr
        }
        binlog.Log(logEntry)
    }

    // comp and cp are used for compression.  decomp and dc are used for
    // decompression.  If comp and decomp are both set, they are the same;
    // however they are kept separate to ensure that at most one of the
    // compressor/decompressor variable pairs are set for use later.
    var comp, decomp encoding.Compressor
    var cp Compressor
    var dc Decompressor

    // If dc is set and matches the stream's compression, use it.  Otherwise, try
    // to find a matching registered compressor for decomp.
    if rc := stream.RecvCompress(); s.opts.dc != nil && s.opts.dc.Type() == rc {
        dc = s.opts.dc
    } else if rc != "" && rc != encoding.Identity {
        decomp = encoding.GetCompressor(rc)
        if decomp == nil {
            st := status.Newf(codes.Unimplemented, "grpc: Decompressor is not installed for grpc-encoding %q", rc)
            t.WriteStatus(stream, st)
            return st.Err()
        }
    }

    // If cp is set, use it.  Otherwise, attempt to compress the response using
    // the incoming message compression method.
    //
    // NOTE: this needs to be ahead of all handling, https://github.com/grpc/grpc-go/issues/686.
    if s.opts.cp != nil {
        cp = s.opts.cp
        stream.SetSendCompress(cp.Type())
    } else if rc := stream.RecvCompress(); rc != "" && rc != encoding.Identity {
        // Legacy compressor not specified; attempt to respond with same encoding.
        comp = encoding.GetCompressor(rc)
        if comp != nil {
            stream.SetSendCompress(rc)
        }
    }

    var payInfo *payloadInfo
    if sh != nil || binlog != nil {
        payInfo = &payloadInfo{}
    }
    d, err := recvAndDecompress(&parser{r: stream}, stream, dc, s.opts.maxReceiveMessageSize, payInfo, decomp)
    if err != nil {
        if e := t.WriteStatus(stream, status.Convert(err)); e != nil {
            channelz.Warningf(logger, s.channelzID, "grpc: Server.processUnaryRPC failed to write status %v", e)
        }
        return err
    }
    if channelz.IsOn() {
        t.IncrMsgRecv()
    }
    df := func(v interface{}) error {
        if err := s.getCodec(stream.ContentSubtype()).Unmarshal(d, v); err != nil {
            return status.Errorf(codes.Internal, "grpc: error unmarshalling request: %v", err)
        }
        if sh != nil {
            sh.HandleRPC(stream.Context(), &stats.InPayload{
                RecvTime:   time.Now(),
                Payload:    v,
                WireLength: payInfo.wireLength + headerLen,
                Data:       d,
                Length:     len(d),
            })
        }
        if binlog != nil {
            binlog.Log(&binarylog.ClientMessage{
                Message: d,
            })
        }
        if trInfo != nil {
            trInfo.tr.LazyLog(&payload{sent: false, msg: v}, true)
        }
        return nil
    }
    ctx := NewContextWithServerTransportStream(stream.Context(), stream)
    reply, appErr := md.Handler(info.serviceImpl, ctx, df, s.opts.unaryInt)
    if appErr != nil {
        appStatus, ok := status.FromError(appErr)
        if !ok {
            // Convert appErr if it is not a grpc status error.
            appErr = status.Error(codes.Unknown, appErr.Error())
            appStatus, _ = status.FromError(appErr)
        }
        if trInfo != nil {
            trInfo.tr.LazyLog(stringer(appStatus.Message()), true)
            trInfo.tr.SetError()
        }
        if e := t.WriteStatus(stream, appStatus); e != nil {
            channelz.Warningf(logger, s.channelzID, "grpc: Server.processUnaryRPC failed to write status: %v", e)
        }
        if binlog != nil {
            if h, _ := stream.Header(); h.Len() > 0 {
                // Only log serverHeader if there was header. Otherwise it can
                // be trailer only.
                binlog.Log(&binarylog.ServerHeader{
                    Header: h,
                })
            }
            binlog.Log(&binarylog.ServerTrailer{
                Trailer: stream.Trailer(),
                Err:     appErr,
            })
        }
        return appErr
    }
    if trInfo != nil {
        trInfo.tr.LazyLog(stringer("OK"), false)
    }
    opts := &transport.Options{Last: true}

    if err := s.sendResponse(t, stream, reply, cp, opts, comp); err != nil {
        if err == io.EOF {
            // The entire stream is done (for unary RPC only).
            return err
        }
        if sts, ok := status.FromError(err); ok {
            if e := t.WriteStatus(stream, sts); e != nil {
                channelz.Warningf(logger, s.channelzID, "grpc: Server.processUnaryRPC failed to write status: %v", e)
            }
        } else {
            switch st := err.(type) {
            case transport.ConnectionError:
                // Nothing to do here.
            default:
                panic(fmt.Sprintf("grpc: Unexpected error (%T) from sendResponse: %v", st, st))
            }
        }
        if binlog != nil {
            h, _ := stream.Header()
            binlog.Log(&binarylog.ServerHeader{
                Header: h,
            })
            binlog.Log(&binarylog.ServerTrailer{
                Trailer: stream.Trailer(),
                Err:     appErr,
            })
        }
        return err
    }
    if binlog != nil {
        h, _ := stream.Header()
        binlog.Log(&binarylog.ServerHeader{
            Header: h,
        })
        binlog.Log(&binarylog.ServerMessage{
            Message: reply,
        })
    }
    if channelz.IsOn() {
        t.IncrMsgSent()
    }
    if trInfo != nil {
        trInfo.tr.LazyLog(&payload{sent: true, msg: reply}, true)
    }
    // TODO: Should we be logging if writing status failed here, like above?
    // Should the logging be in WriteStatus?  Should we ignore the WriteStatus
    // error or allow the stats handler to see it?
    err = t.WriteStatus(stream, statusOK)
    if binlog != nil {
        binlog.Log(&binarylog.ServerTrailer{
            Trailer: stream.Trailer(),
            Err:     appErr,
        })
    }
    return err
}

```

### Receive method parameters

`recvAndDecompress()` read the data frame which contains the RPC method parameters.

- `recvAndDecompress()` calls `p.recvMsg()` to read the data frame payload message.
  - The `p.recvMsg()` is more complex than you can expect.
  - In short, `p.recvMsg()` blocks until `http2Server.HandleStreams()` goroutine sends the data frame to it.
  - Please refer to [Request parameter](parameters.md) for detail.
- After `p.recvMsg()` return, `recvAndDecompress()` decompress the payload message.
  - `recvAndDecompress()` calls `checkRecvPayload()` to check the payload to find some kind of compression error.
  - if the payload is ok and compressed,  `recvAndDecompress()` calls `dc.Do()` or `decompress` to decompress the payload.
  - Please refer to [gRPC over HTTP2](https://github.com/grpc/grpc/blob/master/doc/PROTOCOL-HTTP2.md) for decompression.
  - Now, The `Length-Prefixed-Message` (gRPC request) has been read.

```txt
The repeated sequence of Length-Prefixed-Message items is delivered in DATA frames
* Length-Prefixed-Message → Compressed-Flag Message-Length Message
* Compressed-Flag → 0 / 1 # encoded as 1 byte unsigned integer
* Message-Length → {length of Message} # encoded as 4 byte unsigned integer (big endian)
* Message → *{binary octet}
```

Now, the RPC method parameters is ready. Next, let's discuss `md.Handler()` in detail.

```go
func recvAndDecompress(p *parser, s *transport.Stream, dc Decompressor, maxReceiveMessageSize int, payInfo *payloadInfo, compressor encoding.Compressor) ([]byte, error) {
    pf, d, err := p.recvMsg(maxReceiveMessageSize)
    if err != nil {
        return nil, err
    }
    if payInfo != nil {
        payInfo.wireLength = len(d)
    }

    if st := checkRecvPayload(pf, s.RecvCompress(), compressor != nil || dc != nil); st != nil {
        return nil, st.Err()
    }

    var size int
    if pf == compressionMade {
        // To match legacy behavior, if the decompressor is set by WithDecompressor or RPCDecompressor,
        // use this decompressor as the default.
        if dc != nil {
            d, err = dc.Do(bytes.NewReader(d))
            size = len(d)
        } else {
            d, size, err = decompress(compressor, d, maxReceiveMessageSize)
        }
        if err != nil {
            return nil, status.Errorf(codes.Internal, "grpc: failed to decompress the received message %v", err)
        }
    } else {
        size = len(d)
    }
    if size > maxReceiveMessageSize {
        // TODO: Revisit the error code. Currently keep it consistent with java
        // implementation.
        return nil, status.Errorf(codes.ResourceExhausted, "grpc: received message larger than max (%d vs. %d)", size, maxReceiveMessageSize)
    }
    return d, nil
}
```

### Invoke service method

`processUnaryRPC()` calls `md.Handler()` to invoke the target service method.

- `processUnaryRPC()` prepares the `df` function for `md.Handler()`. `df` unmarshalls the payload data into RPC request parameter.
- The `md` parameter of `processUnaryRPC()` is the `MethodDesc`, which is initialized in [Register service](#register-service).
- the `info` parameter of `processUnaryRPC()` is the `serviceInfo`, which is initialized in [Register service](#register-service).

Now everything is ready,

- When `md.Handler()` is called by `processUnaryRPC()`, actually `_Greeter_SayHello_Handler` is called.
- `_Greeter_SayHello_Handler` calls `GreeterServer.SayHello()`.
- `GreeterServer.SayHello()` finishes the business logic and returns the response type.

Next, let's discuss `s.sendResponse()` in detail.

```go
func _Greeter_SayHello_Handler(srv interface{}, ctx context.Context, dec func(interface{}) error, interceptor grpc.UnaryServerInterceptor) (interface{}, error) {
    in := new(HelloRequest)
    if err := dec(in); err != nil {
        return nil, err
    }
    if interceptor == nil {
        return srv.(GreeterServer).SayHello(ctx, in)
    }
    info := &grpc.UnaryServerInfo{
        Server:     srv,
        FullMethod: "/helloworld.Greeter/SayHello",
    }
    handler := func(ctx context.Context, req interface{}) (interface{}, error) {
        return srv.(GreeterServer).SayHello(ctx, req.(*HelloRequest))
    }
    return interceptor(ctx, in, info, handler)
}

type methodHandler func(srv interface{}, ctx context.Context, dec func(interface{}) error, interceptor UnaryServerInterceptor) (interface{}, error)

// MethodDesc represents an RPC service's method specification.
type MethodDesc struct {
    MethodName string
    Handler    methodHandler
}
```

### Send response header and data

`processUnaryRPC()` calls `s.sendResponse()` to send the `Response-Headers` and `Length-Prefixed-Message` to client.

- `s.sendResponse()` is `Server.sendResponse()`.
- `Server.sendResponse()` accepts the `reply` message returned from the previous steps.
- `Server.sendResponse()` encodes and compresses the reply message.
- `Server.sendResponse()` calls `t.Write()` to send the reply message. `t.Write()` is actually `http2Server.Write()`.
  - `http2Server.Write()` calls `t.WriteHeader()` to send the reply header. `t.WriteHeader()` is `http2Server.WriteHeader()`.
    - `http2Server.WriteHeader()` calls `t.writeHeaderLocked()` to finish the job. `t.writeHeaderLocked()` is `http2Server.writeHeaderLocked()`.
    - `http2Server.writeHeaderLocked()` fills `":status"` and `"content-type"` headers.
    - `http2Server.writeHeaderLocked()` calls `t.controlBuf.executeAndPut()` to send the reply header.
    - Here, `Response-Headers` is sent to client.
  - `http2Server.Write()` creates `dataFrame` based on the reply message and sent it by calling `t.controlBuf.put()`.
  - Here, `Length-Prefixed-Message` is sent to client.
  
For `t.controlBuf.executeAndPut()` and `t.controlBuf.put()`, please refer to  [controlBuffer and loopy](control.md) for detail.

Here is the response example:

```txt
HEADERS (flags = END_HEADERS)
:status = 200
grpc-encoding = gzip
content-type = application/grpc+proto

DATA
<Length-Prefixed Message>

HEADERS (flags = END_STREAM, END_HEADERS)
grpc-status = 0 # OK
trace-proto-bin = jher831yy13JHy3hc
```

Now, the response header and data frame has been sent to client. Next, let's discuss `t.WriteStatus()` in detail.

```go
func (s *Server) sendResponse(t transport.ServerTransport, stream *transport.Stream, msg interface{}, cp Compressor, opts *transport.Options, comp encoding.Compressor) error {
    data, err := encode(s.getCodec(stream.ContentSubtype()), msg)
    if err != nil {
        channelz.Error(logger, s.channelzID, "grpc: server failed to encode response: ", err)
        return err
    }
    compData, err := compress(data, cp, comp)
    if err != nil {
        channelz.Error(logger, s.channelzID, "grpc: server failed to compress response: ", err)
        return err
    }
    hdr, payload := msgHeader(data, compData)
    // TODO(dfawley): should we be checking len(data) instead?
    if len(payload) > s.opts.maxSendMessageSize {
        return status.Errorf(codes.ResourceExhausted, "grpc: trying to send message larger than max (%d vs. %d)", len(payload), s.opts.maxSendMessageSize)
    }
    err = t.Write(stream, hdr, payload, opts)
    if err == nil && s.opts.statsHandler != nil {
        s.opts.statsHandler.HandleRPC(stream.Context(), outPayload(false, msg, data, payload, time.Now()))
    }
    return err
}

// Write converts the data into HTTP2 data frame and sends it out. Non-nil error
// is returns if it fails (e.g., framing error, transport error).
func (t *http2Server) Write(s *Stream, hdr []byte, data []byte, opts *Options) error {
    if !s.isHeaderSent() { // Headers haven't been written yet.
        if err := t.WriteHeader(s, nil); err != nil {
            if _, ok := err.(ConnectionError); ok {
                return err
            }
            // TODO(mmukhi, dfawley): Make sure this is the right code to return.
            return status.Errorf(codes.Internal, "transport: %v", err)
        }
    } else {
        // Writing headers checks for this condition.
        if s.getState() == streamDone {
            // TODO(mmukhi, dfawley): Should the server write also return io.EOF?
            s.cancel()
            select {
            case <-t.done:
                return ErrConnClosing
            default:
            }
            return ContextErr(s.ctx.Err())
        }
    }
    df := &dataFrame{
        streamID:    s.id,
        h:           hdr,
        d:           data,
        onEachWrite: t.setResetPingStrikes,
    }
    if err := s.wq.get(int32(len(hdr) + len(data))); err != nil {
        select {
        case <-t.done:
            return ErrConnClosing
        default:
        }
        return ContextErr(s.ctx.Err())
    }
    return t.controlBuf.put(df)
}

// WriteHeader sends the header metadata md back to the client.
func (t *http2Server) WriteHeader(s *Stream, md metadata.MD) error {
    if s.updateHeaderSent() || s.getState() == streamDone {
        return ErrIllegalHeaderWrite
    }
    s.hdrMu.Lock()
    if md.Len() > 0 {
        if s.header.Len() > 0 {
            s.header = metadata.Join(s.header, md)
        } else {
            s.header = md
        }
    }
    if err := t.writeHeaderLocked(s); err != nil {
        s.hdrMu.Unlock()
        return err
    }
    s.hdrMu.Unlock()
    return nil
}

func (t *http2Server) writeHeaderLocked(s *Stream) error {
    // TODO(mmukhi): Benchmark if the performance gets better if count the metadata and other header fields
    // first and create a slice of that exact size.
    headerFields := make([]hpack.HeaderField, 0, 2) // at least :status, content-type will be there if none else.
    headerFields = append(headerFields, hpack.HeaderField{Name: ":status", Value: "200"})
    headerFields = append(headerFields, hpack.HeaderField{Name: "content-type", Value: grpcutil.ContentType(s.contentSubtype)})
    if s.sendCompress != "" {
        headerFields = append(headerFields, hpack.HeaderField{Name: "grpc-encoding", Value: s.sendCompress})
    }
    headerFields = appendHeaderFieldsFromMD(headerFields, s.header)
    success, err := t.controlBuf.executeAndPut(t.checkForHeaderListSize, &headerFrame{
        streamID:  s.id,
        hf:        headerFields,
        endStream: false,
        onWrite:   t.setResetPingStrikes,
    })
    if !success {
        if err != nil {
            return err
        }
        t.closeStream(s, true, http2.ErrCodeInternal, false)
        return ErrHeaderListSizeLimitViolation
    }
    if t.stats != nil {
        // Note: Headers are compressed with hpack after this call returns.
        // No WireLength field is set here.
        outHeader := &stats.OutHeader{
            Header:      s.header.Copy(),
            Compression: s.sendCompress,
        }
        t.stats.HandleRPC(s.Context(), outHeader)
    }
    return nil
}
```

### Finish stream

`processUnaryRPC()` calls `t.WriteStatus()` send the trailer to end the stream.

- `t.WriteStatus()` is actually `http2Server.WriteStatus()`.
- `http2Server.WriteStatus()` adds headers: `"grpc-status"` and `"grpc-message"` to create `trailingHeader`.
- `http2Server.WriteStatus()` checks the size of `trailingHeader`.
  - Please note `t.controlBuf.execute(t.checkForHeaderListSize, trailingHeader)` only runs `t.checkForHeaderListSize`.
- `http2Server.WriteStatus()` calls `t.finishStream()` to send the `trailingHeader`.
  - `t.finishStream()` is actually `http2Server.finishStream()`.
  - `http2Server.finishStream()` builds the `cleanupStream` and calls `t.controlBuf.put()` to send it to client.
  - Here, the `Trailers` is sent.

`t.WriteStatus()` is the last one to be called in `processUnaryRPC()`. It job is sending stream status to the client and terminates the stream.

For `t.controlBuf.executeAndPut()` and `t.controlBuf.put()`, please refer to  [controlBuffer and loopy](control.md) for detail.

```go
// WriteStatus sends stream status to the client and terminates the stream.
// There is no further I/O operations being able to perform on this stream.
// TODO(zhaoq): Now it indicates the end of entire stream. Revisit if early
// OK is adopted.
func (t *http2Server) WriteStatus(s *Stream, st *status.Status) error {
    if s.getState() == streamDone {
        return nil
    }
    s.hdrMu.Lock()
    // TODO(mmukhi): Benchmark if the performance gets better if count the metadata and other header fields
    // first and create a slice of that exact size.
    headerFields := make([]hpack.HeaderField, 0, 2) // grpc-status and grpc-message will be there if none else.
    if !s.updateHeaderSent() {                      // No headers have been sent.
        if len(s.header) > 0 { // Send a separate header frame.
            if err := t.writeHeaderLocked(s); err != nil {
                s.hdrMu.Unlock()
                return err
            }
        } else { // Send a trailer only response.
            headerFields = append(headerFields, hpack.HeaderField{Name: ":status", Value: "200"})
            headerFields = append(headerFields, hpack.HeaderField{Name: "content-type", Value: grpcutil.ContentType(s.contentSubtype)})
        }
    }
    headerFields = append(headerFields, hpack.HeaderField{Name: "grpc-status", Value: strconv.Itoa(int(st.Code()))})
    headerFields = append(headerFields, hpack.HeaderField{Name: "grpc-message", Value: encodeGrpcMessage(st.Message())})

    if p := st.Proto(); p != nil && len(p.Details) > 0 {
        stBytes, err := proto.Marshal(p)
        if err != nil {
            // TODO: return error instead, when callers are able to handle it.
            logger.Errorf("transport: failed to marshal rpc status: %v, error: %v", p, err)
        } else {
            headerFields = append(headerFields, hpack.HeaderField{Name: "grpc-status-details-bin", Value: encodeBinHeader(stBytes)})
        }
    }

    // Attach the trailer metadata.
    headerFields = appendHeaderFieldsFromMD(headerFields, s.trailer)
    trailingHeader := &headerFrame{
        streamID:  s.id,
        hf:        headerFields,
        endStream: true,
        onWrite:   t.setResetPingStrikes,
    }
    s.hdrMu.Unlock()
    success, err := t.controlBuf.execute(t.checkForHeaderListSize, trailingHeader)
    if !success {
        if err != nil {
            return err
        }
        t.closeStream(s, true, http2.ErrCodeInternal, false)
        return ErrHeaderListSizeLimitViolation
    }
    // Send a RST_STREAM after the trailers if the client has not already half-closed.
    rst := s.getState() == streamActive
    t.finishStream(s, rst, http2.ErrCodeNo, trailingHeader, true)
    if t.stats != nil {
        // Note: The trailer fields are compressed with hpack after this call returns.
        // No WireLength field is set here.
        t.stats.HandleRPC(s.Context(), &stats.OutTrailer{
            Trailer: s.trailer.Copy(),
        })
    }
    return nil
}

// finishStream closes the stream and puts the trailing headerFrame into controlbuf.
func (t *http2Server) finishStream(s *Stream, rst bool, rstCode http2.ErrCode, hdr *headerFrame, eosReceived bool) {
    oldState := s.swapState(streamDone)
    if oldState == streamDone {
        // If the stream was already done, return.
        return
    }

    hdr.cleanup = &cleanupStream{
        streamID: s.id,
        rst:      rst,
        rstCode:  rstCode,
        onWrite: func() {
            t.deleteStream(s, eosReceived)
        },
    }
    t.controlBuf.put(hdr)
}
```
